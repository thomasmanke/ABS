{"cells":[{"cell_type":"markdown","source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/thomasmanke/ABS/blob/main/Notebooks/MarkovChains/MC_002_Properties.ipynb)"],"metadata":{"id":"FkekGi6um5xh"}},{"cell_type":"markdown","metadata":{"id":"eBcpbeidC5tf"},"source":["# Analysing Markov chain properties"]},{"cell_type":"markdown","source":["## Preparations"],"metadata":{"id":"RvBkP1ymcFTV"}},{"cell_type":"markdown","source":["Import Required Modules and Define Convenience Functions"],"metadata":{"id":"36CJgM09bhgx"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def generate_sequence(P, s=0, T=100):\n","\n","  assert P.shape[0]==P.shape[1],         \"generate_sequence: P should be a squared matrix\"\n","  assert np.allclose( P.sum(axis=1), 1), \"generate_sequence: P should be a stochastic matrix\"\n","\n","  ns = P.shape[0] # number of states\n","  L = [s]         # initial state\n","\n","  # loop for T time steps\n","  for t in range(T):\n","    s = np.random.choice( ns, p = P[s, :] )\n","    L.append(s)\n","  return L"],"metadata":{"id":"9T6rVFQ5bMNZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's make sure we start from the same model (transition probabilities)"],"metadata":{"id":"loRGD_8Gaksa"}},{"cell_type":"code","source":["P = np.array(\n","    [[0.8, 0.2], \n","     [0.1, 0.9]]\n",")\n","X=generate_sequence(P)\n","print(*X, sep='')"],"metadata":{"id":"cdIUgzObalUp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eyedJHsprYJy"},"source":["## Probability of a Sequence\n","\n"]},{"cell_type":"markdown","source":["The probability of an observed state sequence $X=(X_1, X_2, \\ldots X_T)=X_{1:T}$ is an important quantity: \n","\n","$$\n","Pr(X|P) = Pr(X_1) Pr(X_2|X_1) Pr(X_3|X_2) \\cdots Pr(X_T|X_{T-1})\n","$$\n","\n","It may be used to classify a given observation $X$ as more or less likely to emerge from different models\n","\n","$$\n","Pr(X|P_1) < Pr(X|P_2) \n","$$"],"metadata":{"id":"sIfGKWKZe4di"}},{"cell_type":"markdown","source":["**Group Task (20 min)**: Strange coins\n","\n","Your favorite casino seems to use very strange coins and you try 2 different Markov models to model the occurence of heads (0) and tails (1). The transition probabilities read \n","$$\n","P_1 = \\begin{bmatrix} \n","0.2   &  0.8 \\\\ \n","0.8   &  0.2 \n","\\end{bmatrix} \n","$$\n","\n","\\\n","\n","$$\n","P_2 = \\begin{bmatrix} \n","0.8   &  0.2 \\\\ \n","0.8   &  0.2 \n","\\end{bmatrix} \n","$$\n","\n","You also assume that the initial probability for observing heads (0) or tails (1) is 50% in both cases.\n","\n","1. Give a verbal account of the model and contrast this with a fair dice.\n","2. Complete the code block below to calculate the probability of the sequence $X=0010000011$ under your model. \n","3. Can you calculate the probability for a fair dice analytically (with a simple formula)?\n","4. Exchange the results with the other group and decide which model is more likely.\n"],"metadata":{"id":"bBwl-jB0hC54"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"g51qgZ8xHr7d"},"outputs":[],"source":["%%script echo Edit before executing\n","X = [0,0,1,0,0,0,0,0,1,1]\n","P = ...\n","\n","prob = ...                     # initial probability: P(x_1=0)\n","# loop over all edges in chain \n","for t in range( ... ):\n","    x1= ...                   # current state\n","    x2= ...                   # next state\n","    prob = prob* ...          # probability update\n","\n","    # you might want to print for debugging\n","    print('t={}, x1={}, x2={} P[s1,s2]={}'.format(t,x1,x2, P[x1,x2]) )\n","\n","print('prob (model)     =', prob)\n","print('prob (fair dice) =', ... formula ? ...)"]},{"cell_type":"markdown","source":["**Follow up:** Duplicate (repeat) the sequence $X$ to obtain a new $X$ with twice the length?  What can you say about the probability?"],"metadata":{"id":"MGXxgpsQtf8h"}},{"cell_type":"markdown","source":["**Messages**\n","- Markov Models are probabilistic models: given the transition matrix $P$ we can calculate many interesting probabilities\n","- More specifically: the probability of a given sequences. Decreases with sequence length - better use logs for long sequences!\n","- Two approaches: Mathematical Treatment of stochastic matrices vs Numerical Simulations (algorithmic efficiency)"],"metadata":{"id":"UDOiqK2ieDXs"}},{"cell_type":"markdown","metadata":{"id":"xxXf-xyvN89Z"},"source":["## counting occurrences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bjew5NxYOCrW"},"outputs":[],"source":["# define new transition matrix\n","P = np.array( [[0.8, 0.2], [0.1, 0.9]] )\n","np.random.seed(42)\n","X = generate_sequence(P, s=0)\n","\n","states, counts = np.unique(X, return_counts=True)\n","B = plt.bar(states.astype(str), counts, align='center', width=0.2)"]},{"cell_type":"markdown","metadata":{"id":"BCmuXb-EZZN1"},"source":["**Task (5 min):** \n","\n","Repeat the above plot for different initial states s and different length of sequences. \n","\n","What do you observe?\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"kg9FQsbgYAXx"},"source":["## Calculating higher order probabilities\n","\n"]},{"cell_type":"markdown","source":["A Markov Model is defined by all 1-step transition probabilities: $P_{ij}$ (for $i \\to j$). "],"metadata":{"id":"l8PLIggIpQey"}},{"cell_type":"markdown","source":["What is the probability for transitions $i \\to j$ in 2 steps, 3 steps, ...t-steps?\n","\n","$$P_{ij}(t) = P(X_t = j | X_0 = i)$$"],"metadata":{"id":"ikuledjoRgUC"}},{"cell_type":"markdown","metadata":{"id":"LL1uJdIYiBW6"},"source":["**Group task (15 min):**  Consider a specific case for the given $P$ above.\n","Starting at $t=0$ in *state i=0*, what are the probabilities at time $t$\n","\n","Group 1:  to be in *state j=0*?\n","\n","Group 2:  to be in *state j=1*?\n","\n","In other words:  Group 1 $\\to$ Calculate $P_{00}(t)$. Group 2 $\\to$ Calculate $P_{01}(t)$.\n","\n","a) at time t=1 ?\n","    \n","b) at time t=2 ?\n","\n","**Hint:** Consider all paths that go from $i \\to j$ "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-88xYRiOiBW6"},"outputs":[],"source":["%%script echo Edit before executing\n","print('P01(t=1): ', ... )\n","print('P01(t=2): ', ... )\n","print('P00(t=1): ', ... ) \n","print('P00(t=2): ', ... ) "]},{"cell_type":"markdown","source":["For larger $t$, these calculations can fast become cumbersome ..."],"metadata":{"id":"Ju87QJrnmYwW"}},{"cell_type":"markdown","source":["### Generalization and Simplification"],"metadata":{"id":"rHIuANrqDKaH"}},{"cell_type":"markdown","source":["Required Tools:\n","1. Matrix Multiplication: $C_{ij} = \\sum_k A_{ik}B_{kj}$\n","2. Marginalization: $P(A) = \\sum_B P(A|B) P(B)$"],"metadata":{"id":"yh8MM62vEvbZ"}},{"cell_type":"markdown","source":["1. **Initialization:**  $P_{ij}(t=0) = \\delta_{ij}$\n","2. **1-step probabilties:**  $P_{ij}(t=1) = P_{ij}$  (Definition of Markov Chain)\n","3. **Recursion:** (marginalization over all states at time $t$)\n","\n","$$\n","\\begin{align}\n"," Pr(X_{t+1}=j|X_0=i) &= \\sum_k Pr(X_{t+1}=j | X_t=k, X_0=i) \\cdot Pr(X_t=k|X_0=i) \\\\ \n"," &= \\sum_k Pr(X_{t+1}=j | X_t=k) \\cdot Pr(X_t=k|X_0=i) \\\\ \n"," P_{ij}(t+1) &= \\sum_k P_{kj} \\cdot P_{ik}(t) = (P^t)_{ij}\n"," \\end{align}\n"," $$\n","\n","\n","$\\sum_k$ is the sum over all connections from state $k$ to $j$, and $p_{ik}(t)$ is the previous result over all possible pathways from $i$ to $k$ "],"metadata":{"id":"rzd27UXwR9C2"}},{"cell_type":"markdown","metadata":{"id":"TGqgiBiAYrkH"},"source":["The above is for the t-step transition matrix. \n","For the probability distribution over states we have:\n","\n","$$\n","\\begin{align}\n"," Pr(X_{t}=j) &= \\sum_k Pr(X_t=j | X_o=k) \\cdot Pr(X_0=k) \\\\ \n","             &= \\sum_k P_{kj}(t) \\cdot Pr(X_0=k)\n"," \\end{align}\n"," $$\n","\n","**Simplification**: (in vector notation)\n","\n","$$\\pi(t) = \\pi(0) \\cdot P^t$$"]},{"cell_type":"markdown","source":["**Task (5min)**: Matrix Power\n","\n","Explore the function matrix_power() and apply this function to $P$ with an increasing value of $t$ "],"metadata":{"id":"HnkleWlHFDCe"}},{"cell_type":"code","source":["%%script echo Edit before executing\n","from numpy.linalg import matrix_power\n","matrix_power( ... )"],"metadata":{"id":"05D2BvLVFJus"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CHCFLR49iBW8"},"source":["<div class=\"alert alert-block alert-warning\">\n","\n","**Task (10 min):** Deja-vu\n","    \n","Starting in state 0 at t=0, calculate the probabilities to be in state 1\n","    \n","a) at time t=1 ?\n","    \n","b) at time t=2 ?\n","    \n","c) at time t=20 ?\n","\n","Solve it using matrix multiplication: https://numpy.org/doc/stable/reference/generated/numpy.linalg.matrix_power.html\n","    \n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"txhkVX4OiBW9"},"outputs":[],"source":["%%script echo Edit before executing\n","from numpy.linalg import matrix_power\n","pi_0 =        # initial probability\n","pi_t =        # dot product of initial probability with P^k"]},{"cell_type":"markdown","metadata":{"id":"ALB14r_AiBW9"},"source":["<div class=\"alert alert-block alert-warning\">\n","\n","**Task (5 min):** \n","\n","Repeat the above question with state 1 at t=0.\n","\n","    \n","a) at time t=1 ?\n","    \n","b) at time t=2 ?\n","    \n","c) at time t=20 ?\n","    \n","\n","**Extra:** Repeat the above with an uncertain state $\\pi=[0.5, 0.5]$\n","    \n","</div>"]},{"cell_type":"markdown","metadata":{"id":"yF7ffFIwONhz"},"source":["**Question:** What is different?"]},{"cell_type":"markdown","metadata":{"id":"nPG6zB0VOM6u"},"source":["**Lessons**: \n","\n","1. Importance of recursions\n","1. Matrix multiplication is powerful tool (and easy with Python/Numpy)! \n","2. Matrix powers of $(P^t)_{ij}$ give the t-step transition probabilities from $i$ to $j$ \n","3. $\\pi(t) = \\pi(0) \\cdot P^t$ gives the state probability distribution at time $t$\n","4. Important role of linear algebra, efficient algorithms and tools\n","5. Convergence and Independence of Initial State "]},{"cell_type":"markdown","metadata":{"id":"TrSiWwiZyoaI"},"source":["## Long-time Properties\n","\n","Given this image of a Markov Chain, can you speculate on its long-time properties?\n","\n","<div>\n","   <img src=\"https://github.com/thomasmanke/ABS/raw/main/figures/MC_AbsorbingGraph.jpg\",  align=left width=\"800\">\n","</div>"]},{"cell_type":"markdown","source":["**Task (5 min):** \n","\n","Let's test it numerically! Define the corresponding transition matrix $P$ and run the code cell below."],"metadata":{"id":"fvv08HRprAJn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wn5AuyJr4l_h"},"outputs":[],"source":["%%script echo Edit before executing\n","P= ....\n","\n","X = generate_sequence(P)\n","print(*X, sep='')\n","\n","states, counts = np.unique(X, return_counts=True)\n","B = plt.bar(states.astype(str), counts, align='center', width=0.2)"]},{"cell_type":"markdown","metadata":{"id":"7cxOfxIDOFCC"},"source":["**Task (5 min)**: What was the initial state? Modify the above code to chose a different initial state. How does the output change?"]},{"cell_type":"markdown","source":["**Lesson:** Some Markov Chains (transition matrices) can have **absorbing states**. More generally there could be multiple absorbing states."],"metadata":{"id":"9S0wQGfzSjoL"}},{"cell_type":"markdown","source":["## Visualizating Matrix Powers"],"metadata":{"id":"usoB9DLcTVBm"}},{"cell_type":"markdown","metadata":{"id":"tiPRxprJO4jh"},"source":["Let's make the temporal analysis more visually pleasing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fe4GI9sHN3Nl"},"outputs":[],"source":["from numpy.linalg import matrix_power\n","fig, ax = plt.subplots(nrows=1, ncols=5, figsize=(10,5))\n","i=0\n","for t in [0,1,2,10,100]:\n","  S=matrix_power(P,t)\n","  im=ax[i].imshow(S, cmap=plt.cm.Blues)\n","  ax[i].set_title(t)\n","  i = i + 1\n","\n","cax = fig.add_axes([0.1, 0.2, 0.8, 0.05])\n","fig.colorbar(im, cax=cax, orientation='horizontal')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"bU3m_FwWVdEq"},"source":["**Discussion**: What do we see here?"]},{"cell_type":"markdown","metadata":{"id":"Gnuf-rVhXhom"},"source":["I'm proud of the above visualization, so let's make it a (slightly more flexible) function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9JeojLU5WUBV"},"outputs":[],"source":["def plot_transition_matrix(P, tmax=100):\n","\n","  # some sanity checks\n","  assert P.shape[0]==P.shape[1],         \"P should be a squared matrix\"\n","  assert np.allclose( P.sum(axis=1), 1), \"P should be a stochastic matrix\"\n","\n","  fig, ax = plt.subplots(nrows=1, ncols=5, figsize=(10,5))\n","  i=0\n","  for t in [0,1,2,10,tmax]:\n","    S=matrix_power(P,t)\n","    im=ax[i].imshow(S, cmap=plt.cm.Blues)\n","    ax[i].set_title(t)\n","    i = i + 1\n","  cax = fig.add_axes([0.1, 0.2, 0.8, 0.05])\n","  fig.colorbar(im, cax=cax, orientation='horizontal')\n","  plt.show()"]},{"cell_type":"markdown","source":["## Convergence Rate"],"metadata":{"id":"61AzM-xhTEAm"}},{"cell_type":"markdown","source":["We saw that the Markov chain above converges to some asymptotic state - let's explore and modify this behaviour."],"metadata":{"id":"KH6fOUGCFZRS"}},{"cell_type":"markdown","metadata":{"id":"7MBilrXqP4Ph"},"source":["**Group Tasks (30min):** \n","Adjust the parameters in $P$ to\n","\n","1. **delay the convergence** to the absorbing state 3.\n","2. generates **two absorbing states**\n","3. avoid absorbing states"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":[],"id":"5ThbITqeP4Pi"},"outputs":[],"source":["%%script echo Edit before eexeecution\n","P1=np.array(...)\n","P2=np.array(...)\n","P3=np.array(...)\n","\n","plot_transition_matrix(P1, 100)\n","plot_transition_matrix(P2, 100)\n","plot_transition_matrix(P3, 100)"]},{"cell_type":"markdown","metadata":{"id":"zuTuFKdsbSM0"},"source":["**Lessons:** The long time behaviour depends on model structure and parameters\n","\n","1. Parameters may control convergence rate\n","2. structural aspects (state accessibilty)may cause multiple absorbing states. The Markov chain will still converge. But $\\to$ initial state sensitivity\n","3. some (interesting) Markov chains have a unique stationary state that is independent from the initial state.\n","\n","**Notice:** \n","\n","- stationary distribution (sometimes called steady state distribution) **does not** mean that the states don't change, but that the probability distribution is constant over time (and independent of the initial state)\n","- the power method is a simple but expensive way to determine the stationary distribution $\\pi$, but there are more effcient ways using the eigenvalue formulation\n","$$\\pi = \\pi \\cdot P$$\n","- this formula also exends the idea of the stationary distribution beyond the long-term behaviour of a Markov chain; a stationary distribution is a distribution that stays fixed after application of $P$ (regardless of whether it can be reached through an asymptotic process)\n","- a Markov chain can have several stationary distributions\n","- Many Markov chains have a unique stationary distribution and they are usually the most interesting/important ones. However, we need to be aware of - and protect against - other possibilities (see below)\n","\n"]},{"cell_type":"markdown","source":["## Periodic States"],"metadata":{"id":"NIS3MlbCn3vw"}},{"cell_type":"markdown","source":["Often we want to avoid periodicity as another \"pathological\" condition."],"metadata":{"id":"uWxOoTvsQL1r"}},{"cell_type":"code","source":["P=np.array([[0.0,1.0,0.0],\n","            [0.5,0.0,0.5],\n","            [0.0,1.0,0.0]])\n","\n","plot_transition_matrix(P, 11)"],"metadata":{"id":"gsxuvstVn6TH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","**Notice:** any self-transition ($P_{ii} \\ne 0$) will result in an aperiodic Markov chain."],"metadata":{"id":"EScwBo45qk-s"}},{"cell_type":"markdown","source":["## Summary \n","\n","We have seen different possible behaviours of Markov chains\n","1. convergence to unique state (stationary or absorbing)\n","2. convergence to non-unique state: inital state sensitivity\n","3. periodic behaviour\n","\n","\"Nice\" Markov chains are \"ergodic\" (everything that can happen will eventually happen) and \"non-periodic\"\n"],"metadata":{"id":"GLDDDNttHZrZ"}},{"cell_type":"markdown","source":["## Expected Times: Convergence and Return"],"metadata":{"id":"idrA5RrcZ3mU"}},{"cell_type":"markdown","source":["### Convergence Time"],"metadata":{"id":"WJNT1vo7aE8f"}},{"cell_type":"markdown","source":["The convergence of a Markov chain $P$ to its stationary distribution $\\pi$ is a very important property. \n","\n","Starting from state $\\pi_0=(1, 0)$ the approximation to the stationary distribution\n","is given by\n","\n","$$\n","\\pi_t = \\pi_0 P^t\n","$$\n","\n","The distance from the stationary distribution is given by the Euclidean norm\n","$$\n","\\delta(t) = ||\\pi_t - \\pi||\n","$$\n","\n","see also numpy: np.linalg.norm()"],"metadata":{"id":"IoD6u3V74PiF"}},{"cell_type":"markdown","source":["**Task (10 min)**: \n","\n","Asymptotically the Markov chain defined by $P$ (above) has stationary distribution $\\pi=(1/3, 2/3)$. Calculate the distance of $\\delta(t)$ and plot it logarithmically against $t=1 \\ldots 50$."],"metadata":{"id":"FAqmWd3l55Be"}},{"cell_type":"code","source":["%%script echo Edit before executing\n","from numpy.linalg import matrix_power\n","\n","P  = np.array([[0.8, 0.2],[0.1,0.9]])  # transition matrix\n","pi = np.array([1./3, 2./3])            # stationatry distribution\n","T  = 50                                # maximal number of times\n","\n","pi_0 = np.array([1,0])                 # initial distribution\n","\n","dist = np.zeros(T)                     # initialize T distances\n","for t in range(T):\n","  pi_t =        # hint: P^t\n","  dist[t] = ... # calculate distance --> hint: np.linalg.norm\n","#  print(pi_t, dist[t])\n","\n","plt.plot(dist)\n","plt.yscale('log')\n","plt.xlabel('time')\n","plt.ylabel('distance from stationary')\n","plt.show()"],"metadata":{"id":"c6l2Finu0X3t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Message**: The convergence is exponential. The precise rate is controlled by matrix properties of $P$ (its eigenvalues)."],"metadata":{"id":"UJ0lYpbq5iVT"}},{"cell_type":"markdown","source":["### State Duration\n","\n","How long is the expected stay $E[t_i]$ in a given state $i$?"],"metadata":{"id":"a7d-wT9JgaYV"}},{"cell_type":"markdown","source":["$$\n","Pr(X_1=i, X_2=i, \\ldots X_{t}=i, X_{t+1} \\ne i) = p_{ii}^{t-1} (1-p_{ii}) \\equiv p_i(t) \\\\ \\\\\n","E[t_i] = \\sum_{t=1}^\\infty t p_i(t) = (1-p_{ii}) \\sum_t t p_{ii}^{t-1} = \\frac{1}{1-p_{ii}}\n","$$"],"metadata":{"id":"Aic8mxrHgwEo"}},{"cell_type":"markdown","source":["**Task (10 min):** Define your favorite Markov Model (transition matrix) and generate a (sufficiently) long sequence from it. Pick a specific state $i$ and confirm the above formula. A simple helper function is provided below, but try to understand it."],"metadata":{"id":"W_jl6iRbiQb2"}},{"cell_type":"code","source":["def StateDuration(L, s=0):\n","  # helper function to calculate the \"duration\" of a given element s in list L\n","  # --> calculate the length of consecutive occurences\n","  count=0\n","  res=[]\n","  for e in L:\n","    if e==s:\n","      count +=1\n","    else:\n","      if (count > 0):\n","        res.append(count)\n","      count=0\n","\n","  if (count>0):\n","    res.append(count)\n","  return res"],"metadata":{"id":"wYV1f6zIlJ0p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%script echo Edit before executing\n","P = np.array( ... your choice here)  # chose transition matrix\n","i=0                                   # chose states\n","\n","X=generate_sequence(P, T= ... )\n","\n","exp = ... # expected duration\n","res = StateDuration(...)\n","print('expected duration: {} average: {}'.format(exp, sum(res)/len(res)))"],"metadata":{"id":"1Q1vGS29iaKi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Parameter Estimation"],"metadata":{"id":"UCPqgKSSgk2Y"}},{"cell_type":"markdown","source":["Given transition rates $P_{ij}$ we can calculate (or simulate) many interesting quantities (path probabilities, stationary states, absorbing states, mixing times, ...).\n","\n","How can we estimate (learn) these model parameters?\n","\n","Frequency interpretation:\n","\n","$$\n","P_{ij} = \\frac{N([s_i, s_j])} {N(s_i)}\n","$$\n","\n","\n","Remember: All models are wrong, some are useful."],"metadata":{"id":"4rfKNz30To0V"}},{"cell_type":"markdown","source":["**Group Task (15 min):** Given a sequence of states. Complete the code below to obtain the parameters of the Markov Chain."],"metadata":{"id":"qC3u7vVSIpbK"}},{"cell_type":"code","source":["%%script echo Edit before executing\n","import numpy as np\n","X=[0,1,1,1,0,1,0]\n","L= ...      # length of observed sequence\n","N=2         # number of states \n","\n","P = np.zeros((N,N))\n","for i in range( .... ):\n","\n","  [ ... fill P_ij_ ...]\n","\n","# normalize rows\n","P = P/np.sum(P,axis=1, keepdims=True)\n","print('estimated transition matrix: \\n', P)"],"metadata":{"id":"pyqdq3oRJDBm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Question:** How would you estimate the intial probabiltiy?"],"metadata":{"id":"9_MPVEXSR5Wz"}}],"metadata":{"colab":{"collapsed_sections":["GLDDDNttHZrZ"],"name":"MC_002_Properties.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":0}
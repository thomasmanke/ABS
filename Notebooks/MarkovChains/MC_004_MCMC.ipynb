{"cells":[{"cell_type":"markdown","metadata":{"id":"sTChvy88nj5O"},"source":["# Markov Chain Monte Carlo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b-Pgk6EjL5MS"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from scipy.stats import norm\n","\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"-Y5lWDPILEgM"},"source":["## A strange distribution - and a simple proposal"]},{"cell_type":"markdown","metadata":{"id":"XRIOznw6Ulvg"},"source":["We have learned how to sample values $x_i$ from simple, well-known distributions (Uniform, Normal, Poisson, Binomial, ...)\n","\n","$$\n","x_i \\propto p(x)\n","$$\n","\n","Python/numpy has several convenience functions, and so have many other packages and programming language: R, matlab, ...\n","\n","But imagine we want to sample from some **arbitrary** probability density function, $p(x) = f(x) /Z$.\n","\n","We assume that function $f(x)\\ge 0$ can be calculated easily, but the normalization factor $Z$ is unknown:\n","\n","$$Z = \\int f(x) dx$$ \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZYVItbymLHbz"},"outputs":[],"source":["def func(x):\n","  # invented some function: f(x)>= 0\n","  y = (1+np.sin(x))*np.exp(-np.abs(x))\n","  return y\n","\n","def prop(x, mu=0, sigma=1):\n","  # define some well-known proposal distribution, e.g. Normal(x|mu, sigma)\n","  return norm.pdf(x, loc=mu, scale=sigma)\n","\n","x=np.linspace(-5,5,1000)\n","plt.fill_between(x, func(x),color='b',alpha=0.5, label='distribution of interest')\n","plt.fill_between(x, prop(x,mu=2,sigma=1), color='r', alpha=0.5, label='proposal distribution')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"RM7BdIyjy7B3"},"source":["**Sampling Goal:** \n","\n","Draw samples $x_i$ from $p(x)$, such that the expectation of any function $A(x)$ can be approximated\n","$$\n","E[A(x)] = \\sum_x A(x) p(x) \\to \\sum_i A(x_i) \n","$$"]},{"cell_type":"markdown","metadata":{"id":"qbPo5G6vSFg8"},"source":["There are a range of sampling methods\n","\n","- Inverse Transform Sampling\n","- Importance Sampling\n","- Accept-Reject Sampling\n","- **Markov Chain Monte Carlo (MCMC)**"]},{"cell_type":"markdown","metadata":{"id":"qe4ILFVkWIIL"},"source":["## The Metropolis Algorithm"]},{"cell_type":"markdown","metadata":{"id":"x6WjcrhhWbkg"},"source":["This is the classical MCMC method (1953)\n","\n","**The idea / The goal:**\n","\n","Sample a sequence of $x_{t=1, ..., T}$ from a **Markov Chain** with the desired stationary distribution $\\pi(x)=f(x)/Z$:\n","$$\n","x_t \\propto MC\n","$$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rF_g1TQ32Z7P"},"source":["**The Algorithm:**\n","1. pick a random start $x_{t=0}$\n","2. propose next $x_p$ from a simpler, feasible distribution, e.g. $N(x|\\mu=x_t, \\sigma^2)$\n","3. accept the proposed sample ($x_{t+1}=x_p$) with probability $f(x_p) / f(x_t)$,  otherwise ($x_{t+1}=x_t$)"]},{"cell_type":"markdown","metadata":{"id":"RiJHvJ4W2Uiv"},"source":["**Notice:**\n","- samples $x_t$ are not *iid*, but are correlated by a random walk: $x_t \\to x_{t+1}$ \n","\n","- a Markov chain with continuous variable\n","- for illustration: $x_t \\in \\mathbb{R}$; in practice: $x_t \\in \\mathbb{R}^n$\n","- Metropolis, Rosenbluth, Rosenbluth, Teller, Teller (J. Chem Phys, 1953) - 48k citations\n"," - https://www.osti.gov/servlets/purl/4390578\n"," - https://cs.gmu.edu/~henryh/483/top-10.html"]},{"cell_type":"markdown","metadata":{"id":"8GldPTpLAX3M"},"source":["## Group Task (30 min)"]},{"cell_type":"markdown","metadata":{"id":"noyfD_16KgBH"},"source":["1. Implement the Metropolis algorithm as a function (below) and test it on the function $f(x)$ above.\n","2. Optional: Invent your own pdf (up to normalization)\n","3. Explore how the initial value ($x$) and the number of samples ($T$) effect the sampled distribution\n","4. Discussion: How could you test or change the convergence properties of the Metropolis algorithm?\n","5. Report your results to the other group"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qLFoEDEbxCX7"},"outputs":[],"source":["%%script echo Edit before running cell\n","def MetropolisSampling(func, x=0, T=1000):\n","  \"\"\" this is a simple implementation of the Metropolis algorithm \n","  input:\n","    - func: a probability density function (not necessarily normalized)\n","    - x: a starting value for x (default: x=0)\n","    - T: number of samples (default: T=1000)\n","  output:\n","    - list of sampled values x\n","  \"\"\"\n","  sample=[] # initialize empty list of values\n","  for t in range(T):\n","     # given \"state\" x, propose a new state xp \n","     # chose normal distribution centered around x\n","    xp = .... \n","\n","     # define the acceptance ratio as a ratio f(xp)/ f(x)\n","    alpha = ...\n","\n","    # accept the proposal if alpha is bigger than a random number in (0,1)  \n","    if (...):\n","      x = xp\n","\n","    # add proposed x to sample list\n","    sample.append(x)\n","\n","  return sample\n","\n","sample = MetropolisSampling(func, x=0, T=2000)\n","\n","# plot samples (\"traces\") and histogram \n","fig, ax = plt.subplots(1, 2)\n","fig.tight_layout()\n","ax[0].plot(sample)\n","ax[0].set_title('Trace Plot')\n","ax[1].hist(sample,100)\n","ax[1].set_title('Histogram')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"uzyWPBc5WKSd"},"source":["## Why does it work?"]},{"cell_type":"markdown","metadata":{"id":"g2l3COHX-Zie"},"source":["**Claim**: We constructed an ergodic Markov chain $X_t \\to  X_{t+1}$\n","\n","**Question**: What are the transition probabilities $P(X_{t+1}=b | X_t=a)$?"]},{"cell_type":"markdown","metadata":{"id":"cOOQxzsD-3b4"},"source":["Write $P$ as product of a **proposal probability** $N$ and an **acceptance probability** $A$\n","$$\n","Pr(X_{t+1}=b | X_t=a) = A(X_{t+1}=b|X_t=a, X_p=b) ~ N(X_p=b|\\mu=a, \\sigma^2)\n","$$\n","\n","$$\n","Pr(b | a) = A(b|a, b) ~N(b|a, \\sigma^2) \\\\\n","Pr(b | a) = \\min\\left(1, \\frac{f(b)}{f(a)}\\right) ~ N(b|a, \\sigma^2) = \\min\\left(1, \\frac{\\pi(b)}{\\pi(a)}\\right) ~ N(b|a, \\sigma^2)\n","$$"]},{"cell_type":"markdown","metadata":{"id":"GbiDqj8vcE03"},"source":["**Detailed balance:** $Pr(a\\to b) = Pr(b\\to a)$\n","\n","$$\n","\\begin{align}\n","\\pi(a)~P(b|a)  &= \\pi(a)~ \\min\\left(1, \\frac{\\pi(b)}{\\pi(a)}\\right) ~ N(b|a, \\sigma^2) \n","           = \\min\\left(\\pi(a), \\pi(b)\\right) ~ N(b|a, \\sigma^2)\\\\\n","           &= \\pi(b) ~ P(a|b)\n","\\end{align}\n","$$\n","\n","Symmetries:\n","\n","- $N(a|b) = N(b|a)$\n","- $\\min(\\pi(a), \\pi(b)) = \\min(\\pi(b),\\pi(a))$\n"]},{"cell_type":"markdown","metadata":{"id":"5ADE4qCPWc5a"},"source":["Notice:\n","- other requirements: $P$ irreducible & aperiodic.\n","- Detailed balance also holds for other symmetric proposal distributions: $Q(a|b)=Q(b|a)$\n","- unknown normalization $Z$ cancels in the acceptance rate.\n"]},{"cell_type":"markdown","metadata":{"id":"NpllQS3uobll"},"source":["## Issues"]},{"cell_type":"markdown","metadata":{"id":"Fzm-iJpDodzU"},"source":["- \"thermalization\": it will take some time before Markov Chain samples the desired stationary distribution\n","- chain onvergence: \n","  - theoretically considerations (rapid mixing), and\n","  - practical approaches (run multiple chains)\n","- autocorrelation: try to make large steps (e.g. $\\sigma^2$ large)\n","- curse of dimensionality: small steps (move rapidly away from high probability regions in $D$ dimensions)"]},{"cell_type":"markdown","metadata":{"id":"jbBCM5BUpT1Y"},"source":["## Many Developments"]},{"cell_type":"markdown","metadata":{"id":"aNV83VPqpWTv"},"source":["- asymmetric proposal distributions: Metropolis-Hastings\n","- Gibbs sampler: less random sampling\n","- Hamiltonian Monte Carlo: model dynamics in parameter space (introduce momentum)\n","- Tools: Stan, pyStan, pymc (3/4)"]},{"cell_type":"markdown","metadata":{"id":"5y982s6BGSqo"},"source":["## Tools: Pymc / Arviz"]},{"cell_type":"markdown","metadata":{"id":"HOfolMpQHIWM"},"source":["- Pymc provides the above functionality (and much more).\n","\n","- Arviz adds statistical analysis and diagnostics."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mdS_CKgfMESH"},"outputs":[],"source":["%%script echo Install only once (if necessary)\n","pip install pymc arviz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nM2zv8QwpVaP"},"outputs":[],"source":["import pymc as pm\n","import arviz as az\n","\n","print(pm.__version__)"]},{"cell_type":"markdown","metadata":{"id":"zmvReHGAr-G4"},"source":["**Notice:** Make sure to use version 4 or higher. There are differences with pymc3, which uses different data formats and defaults."]},{"cell_type":"markdown","metadata":{"id":"Bb7bLIqaXz-b"},"source":["## Metropolis Sampling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a39Zu6WFo9vJ"},"outputs":[],"source":["T=5000 # number of time steps (samples)\n","nc=2   # number of chains\n","\n","# first define log(p) of the function (p>0) to sample from\n","def log_func(x):\n","  return np.log(func(x))\n","\n","# with context manager\n","with pm.Model():\n","  pm.DensityDist('my_x', logp=log_func)     # define custom distribution\n","  trace = pm.sample(T, chains=nc, step=pm.Metropolis())\n","\n","pt=pm.plot_trace(trace)"]},{"cell_type":"markdown","metadata":{"id":"ma0cAo-BvFw2"},"source":["### Autocorrelation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_O21H5NIQ0VG"},"outputs":[],"source":["# access autocorrelation\n","ac=pm.autocorr(np.array(trace.posterior.my_x))\n","plt.plot(ac[0,:100]) # chain=0\n","plt.plot(ac[1,:100]) # chain=1\n","plt.show()\n","\n","# plot autocorrelation (for each chain)\n","ap=pm.plot_autocorr(trace)"]},{"cell_type":"markdown","metadata":{"id":"3TFGbc3PtBm5"},"source":["## References"]},{"cell_type":"markdown","metadata":{"id":"FGYf_O1_tGO3"},"source":["- MCMC Theory (from pymc perspective): https://pymcmc.readthedocs.io/en/latest/theory.html\n","- Metropolis-Hastings: https://stephens999.github.io/fiveMinuteStats/MH_intro.html\n"]}],"metadata":{"colab":{"collapsed_sections":["uzyWPBc5WKSd","NpllQS3uobll","jbBCM5BUpT1Y"],"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":0}
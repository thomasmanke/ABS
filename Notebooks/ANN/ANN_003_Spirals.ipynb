{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ANN_003_Spirals.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPNQ3mSKf1Wo/Tp85d4M3Fb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# More difficult problems"],"metadata":{"id":"ICYzUilD_10I"}},{"cell_type":"markdown","source":["# Load Tools"],"metadata":{"id":"6vX99Mc-iW9E"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from sklearn.metrics import confusion_matrix\n","\n","def plot_cm(mat):\n","  classes = np.arange(cm.shape[0])\n","  plt.imshow(mat, cmap=plt.cm.Blues)\n","  for (j,i),label in np.ndenumerate(mat):\n","    plt.text(i,j,np.round(label,2),ha='center',va='center')\n","\n","  plt.colorbar()\n","  plt.title('Confusion Matrix')\n","  plt.xlabel('True label')\n","  plt.ylabel('Pred label')\n","  plt.xticks(classes)\n","  plt.yticks(classes)\n","  plt.show()"],"metadata":{"id":"0gkuOLJKiUju"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Mock Data"],"metadata":{"id":"JTe13PbiB9fY"}},{"cell_type":"code","source":["N = 100 # number of points per class\n","K = 3   # number of classes\n","\n","X = np.zeros((N*K, 2)) # data matrix (each row = single example)\n","y = np.zeros(N*K, dtype='uint8') # class labels\n","for j in range(K):\n","  ix = range(N*j,N*(j+1))\n","  r = np.linspace(0.0,1,N) # radius\n","  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n","  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n","  y[ix] = j\n","\n","print('Shapes: X= {} y = {} '. format(X.shape, y.shape))\n","plt.scatter(X[:, 0], X[:, 1], c=y)\n","plt.show()"],"metadata":{"id":"Cr7cqSZzBJkv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The main challenge with such data is the *non-linear* structure. Therefore we need some non-linear tools"],"metadata":{"id":"tPd40KyvgI74"}},{"cell_type":"markdown","source":["**Quiz**: How many variables? How many labels? How many samples ?"],"metadata":{"id":"XWtB1YsJM5Ab"}},{"cell_type":"markdown","source":["# From Neurons to Networks"],"metadata":{"id":"x0PO35jLCCdZ"}},{"cell_type":"markdown","source":["## Adding Layers and Non-Linearities"],"metadata":{"id":"cRG0eZLYqOcL"}},{"cell_type":"markdown","source":["<div>\n","   <img src=\"https://github.com/thomasmanke/ABS/raw/main/figures/ANN_ReLU.jpg\",  width=\"1200\">\n","</div>"],"metadata":{"id":"iGa0lYs-zuWR"}},{"cell_type":"code","source":["nc = 3 # number of classes\n","loss_name='sparse_categorical_crossentropy'   # for integer labels\n","acc='sparse_categorical_accuracy'             # additional metrics to track\n","\n","model = tf.keras.Sequential()\n","model.add(tf.keras.layers.Dense(8, input_dim=2))\n","#model.add(tf.keras.layers.Activation('relu'))\n","model.add(tf.keras.layers.Dense(nc, activation='softmax'))\n","\n","model.compile(optimizer='sgd', loss=loss_name, metrics=[acc])\n","model.summary()"],"metadata":{"id":"SfH-SInuCPq8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fh = model.fit(X,y, epochs=300, verbose=0)\n","plt.plot(fh.history['loss'][1:])\n","plt.plot(fh.history[acc][1:])\n","plt.show()"],"metadata":{"id":"a-4jCjPJhM-8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["eval = model.evaluate(x=X, y=y, verbose=0)\n","print('[loss, accuracy] = ', eval)\n","\n","yp = model.predict(X)\n","yp1 = np.argmax(yp, axis=1)\n","\n","cm = confusion_matrix(yp1, y)\n","plot_cm(cm)"],"metadata":{"id":"3QUmx33PiG0k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Saving Models"],"metadata":{"id":"B1enNTvn0N0t"}},{"cell_type":"markdown","source":["Fitting is expensive. It's good practice to save good models. They can be shared or reloaded later - with all parameters in place."],"metadata":{"id":"H89uUTqB0RwS"}},{"cell_type":"code","source":["# save: will create a directory of specified name\n","model.save('model2')\n","\n","# load\n","model2 = tf.keras.models.load_model('model2')"],"metadata":{"id":"JUgUP_Ew0aEy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Group Task (30 min): Changing and comparing models"],"metadata":{"id":"ESm3lBe-n05K"}},{"cell_type":"markdown","source":["You may have to rerun the fit for a more decent performance than in the lecture.\n","\n","Make sure have saved your best model as \"model1\""],"metadata":{"id":"BOCdrXP1oCf2"}},{"cell_type":"markdown","source":["- **Group 1:** **Remove the non-linear activation layer** from the model and save the resulting model as model2. \n","\n","- **Group 2:** **Add an additional layer** with non-linear activation function (e.g. 'relu') before the final output layer and save the resulting model as model 2.\n","\n","Report your results back to all. Summarize the model. What can you say about the performance on the training data? \n","\n","The code cell below should be ready to go without further editing. It may help to visualize differences between the two models, but it requires two distinct models named \"model1\" and \"model2\"\n","\n","\n"],"metadata":{"id":"_CnBWDMLKVvJ"}},{"cell_type":"markdown","source":["### Decision Boundaries"],"metadata":{"id":"vlCxoBsPYIej"}},{"cell_type":"code","source":["# Vizualizing Decision Boundaries\n","# This is feasible only for 2D data in higher dim it becomes useless\n","from mlxtend.plotting import plot_decision_regions\n","\n","# Trick from http://rasbt.github.io/mlxtend/user_guide/plotting/plot_decision_regions/\n","# see Example 12\n","\n","class Onehot2Int(object):\n","    def __init__(self, model):\n","        self.model = model\n","\n","    def predict(self, X):\n","        y_pred = self.model.predict(X)\n","        return np.argmax(y_pred, axis=1)\n","\n","\n","plt.figure(figsize=(12, 5))\n","\n","model1 = tf.keras.models.load_model('model1')\n","model1_adj = Onehot2Int(model1)\n","ax = plt.subplot(1, 2, 1)\n","plot_decision_regions(X, y, clf=model1_adj, legend=2)\n","plt.title('Model 1')\n","\n","model2 = tf.keras.models.load_model('model2')\n","model2_adj = Onehot2Int(model2)\n","ax = plt.subplot(1, 2, 2)\n","plot_decision_regions(X, y, clf=model2_adj, legend=2)\n","plt.title('Model 2')\n","\n","plt.show()"],"metadata":{"id":"iNgcn84LRQT2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Lesson**: \n","The problem was more difficult because the data has non-linear patterns.\n","\n","- Simple non-linear activation functions may help to separate more complex data structures.\n","- We need to protect against over-fitting"],"metadata":{"id":"17nhuuhAYFpk"}},{"cell_type":"markdown","source":["# Summary of the first part"],"metadata":{"id":"Sy1xZwDVSh2a"}},{"cell_type":"markdown","source":["<div>\n","   <img src=\"https://github.com/thomasmanke/ABS/raw/main/figures/ANN_001_Summary.jpg\",  width=\"1000\">\n","</div>"],"metadata":{"id":"MpShBKqMGfsJ"}}]}
{"cells":[{"cell_type":"markdown","metadata":{"id":"howMdVCwrP1D"},"source":["# Motivation and Simulation"]},{"cell_type":"markdown","metadata":{"id":"NKNxmVkcrS1q"},"source":["Even if the \"states of the world\" are Markovian, they are often hidden from us, and we only observe some measurements. \n","\n","**A traveling analogy**\n","\n","> I frequently commute between two states: Germany and Switzerland. Let's assume my travels can be modelled as a Markov Process, as described in the previous section. But now I only communicate my dinner plans with the world. Therefore dinner is an **observable** variable, but my current state (the country) variable is **hidden.** We might hope that something could still be learned about the states visited from the observation on food consumption.\n","\n","<div>\n","   <img src=\"https://github.com/thomasmanke/ABS/raw/main/figures/HMM_CountryFood.jpg\",  width=\"1000\">\n","</div>\n","\n","\n","This is a Hidden Markov Model (HMM). An HMM is characterized by three ingredients:\n","\n","- initial distribution: $P(Z_0=i)=\\pi_i$ ( $\\to 1 x N$ matrix = row vector )\n","- transition matrix: $P(Z_t=j|Z_{t-1}=i) = P_{ij}$  ( $\\to N \\times N$ matrix )\n","- emission matrix: $P(X_t=k|Z_t=i) = E_{ik}$ ( $ \\to N \\times M$ matrix )\n","\n","The emission probabilities are dependent on the state, but constant over time.\n","\n","For simplicity we will assume that both states and observables are discrete.\n","To be specific, the Hidden Markov Model with 2 states $Z \\in$ {Germany=0, Switzerland=1} and observations with 3 possible observations $X \\in$ {Bread=0, Fish=1, Fondue=2} may read:\n","\n","\\begin{align}\n","    P(Z_0) &= \\begin{bmatrix} 0.75 & 0.25  \\end{bmatrix} \\\\ \\\\\n","    P(Z_t | Z_{t-1}) & = \\begin{bmatrix} 0.8 & 0.2 \\\\ 0.1 & 0.9 \\end{bmatrix} \\\\ \\\\\n","    P(X_t | Z_t) & =  \\begin{bmatrix} 0.7 & 0.2 &  0.1 \\\\ 0.1 & 0.1 & 0.8 \\end{bmatrix} \\\\\n","\\end{align}\n","\n","\n","**Discussion:** Give an interpretation of the numbers as they relate to the graph above.  \n","\n","**Notice:** all rows are non-negative and they sum to 1 (*stochastic* matrices)\n","\n","The cell below specifies all these parameters in Python/Numpy.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QfQwDUSJ5-fI"},"outputs":[],"source":["import numpy as np\n","pi=np.array( [0.75, 0.25] )                          # initial state probability\n","P =np.array([ [0.8, 0.2], [0.1, 0.9] ])              # transition probabilites\n","E =np.array([ [0.7, 0.2, 0.1], [0.1, 0.1, 0.8] ])    # emission probabilities"]},{"cell_type":"markdown","metadata":{"id":"q4AcKuQ-D64O"},"source":["**Task (20 min):** Simulate the above Hidden Markov Model. \n","\n","Complete the following function and generate observations from a Hidden Markov Model defined above. You might want to refer back to the first lecture on simple Markov Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KLBgT-663Xcp"},"outputs":[],"source":["# notice the similarities with generate_sequence() from the plain Markov Model\n","def generate_HMM(P, pi, E, T=50):\n","\n","  assert P.shape[0]==P.shape[1],         \"generate_HMM: P should be a squared matrix\"\n","  assert np.allclose( P.sum(axis=1), 1), \"generate_HMM: P should be a stochastic matrix\"\n","  assert np.allclose( E.sum(axis=1), 1), \"generate_HMM: E should be a stochastic matrix\"\n","  assert np.isclose( pi.sum(), 1),       \"generate_HMM: pi should sum to 1\"\n","  assert E.shape[0]==P.shape[0],         \"generate_HMM: E and P should have the same number of rows (states)\"\n","  \n","  # first define two list (states = integers, emissions = letters)\n","  ns = ...                          # number of states\n","  ne = ...                          # number of outputs (#observables)\n","  states= list(range(ns))                  # state labels as integers\n","  emissions=list(range(ne))                # observation labels as integers\n","  #emissions=[chr(97+i) for i in range(ne)] # observation labels as letters\n","\n","  # chose first state and the corresponding emission\n","  # we used to set this deterministically before - here we make a random choice\n","  z = np.random.choice( states,   ... )\n","  x = np.random.choice( emissions, ... )\n","\n","  # add state and observation to history\n","  state_hist = [z]\n","  emit_hist = [x]\n","  \n","  # loop for T time steps\n","  for t in range(T):\n","    z = np.random.choice( states,    ...  )\n","    x = np.random.choice( emissions, ... )\n","\n","    # collect history with state and emission labels\n","    state_hist.append(z)\n","    emit_hist.append(x)\n","  return state_hist, emit_hist"]},{"cell_type":"markdown","metadata":{"id":"WMzW0gd_OMfC"},"source":["**Test:** If done correctly, the function should return output such as"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":278,"status":"ok","timestamp":1654803480394,"user":{"displayName":"Thomas Manke","userId":"17591636328965298454"},"user_tz":-120},"id":"8B8dzc_y-HcV","outputId":"7a526f41-dc1a-4be9-bed9-7c090cea2b95"},"outputs":[{"output_type":"stream","name":"stdout","text":["states Z       = 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0\n","observations X = 2 0 0 1 1 2 2 1 2 2 1 2 2 2 0 1 2 2 0 2 2 2 0 0 0 2 2 2 0 0 0 2 2 2 2 0 1 1 0 0 2 0 2 2 2 2 2 2 2 0 0\n"]}],"source":["np.random.seed(42)\n","Z, X = generate_HMM(P,pi,E, T=50)\n","print('states Z       =',*Z)\n","print('observations X =',*X)"]},{"cell_type":"markdown","metadata":{"id":"WNpoLPjgC5Vk"},"source":["**Discussion:** Do these sequences make sense? Can you give an  interpretation of the observation?\n","\n","From now on we assume that the *hidden* state sequence $Z=Z_{1:T}$ is never observed. \n","\n","However, if we know the HMM parameters, we can still give a probabilitic description for $Z$:\n","\n","- **prior probability** $Pr(Z_t=i)‚ü∂$ stationary distribution $\\pi = \\pi P$ \n","- **posterior probability** $Pr(Z_t=i|X_t=k) \\longrightarrow$ Bayesian update: $ \\propto Pr(X_t=k|Z_t=i) Pr(Z_t=i)$\n","- **most likely hidden state sequence** for given observations: $argmax_Z P(Z|X) \\longrightarrow$ Viterbi algorithm\n","\n","\n","\n"," "]},{"cell_type":"markdown","source":["## Bayesian interlude here"],"metadata":{"id":"fSZYk78UAFcs"}},{"cell_type":"markdown","source":["**Tasks (20 min)**: For the following assume that all HMM parameters are known: $\\pi, P, E$.\n","\n","1. Is the initial distribution the same as the stationary distribution?\n","\n","2. Let's assume that I sent you my (first ever) message, saying that I just had Fondue for dinner. What is the (posterior) probability that I am in Germany?\n","\n","- 10%\n","- 27%\n","- 75%"],"metadata":{"id":"iiCGQY-Ns3wY"}},{"cell_type":"code","source":["# 1. matrix powers\n","from numpy.linalg import matrix_power\n","pi = np.array([1.0, 0.0])         \n","stat_dist =    # independent of pi\n","print('stat_dist = ', stat_dist)\n","\n","# 2.  Bayesian analysis\n","# it will help to write down Bayes formula here\n","sum  = ...\n","prob = ... / sum\n","print('sum = ', sum)\n","print('answer = ', prob)"],"metadata":{"id":"iPK8t7JGwBdo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Later you will learn how to incorporate all observations $X$ systematically to derive probabilitic statements for $Z$."],"metadata":{"id":"m7LwkNjQxlXN"}},{"cell_type":"markdown","metadata":{"id":"e_-ZrBAZ08wp"},"source":["# Group Task (15 + 10 min): HMM Generation"]},{"cell_type":"markdown","metadata":{"id":"zhG8u7wX1BjT"},"source":["1. Make up your own hidden Markov story, draw the corresponding state graph, and define the Hidden Markov Model. \n","  - Please keep it simple; less than 5 hidden states and less than 5 possible observations. \n","  - Also make sure that the hidden states are *ergodic* (what was that?)\n","\n","2. Choose your own emission probabilties, transition probabilties and initital state distribution - make sure they correspond to probabilties. \n","\n","3. Simulate $T=1000$ steps.\n","\n","4. Record (only) the sequence of observations that were generated and store the results as string in a text file (for latter use). Be kind and use integer encoding of observations, i.e. $0,1,\\ldots$ regardless of the interpretation.\n","\n","5. Share your story, code, results and report back to the class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DaVQuGjh6Yey"},"outputs":[],"source":["#%%script echo complete (only one per group)\n","\n","pi=np.array( ... )\n","P =np.array( ... )\n","E =np.array( ... )\n","\n","T=1000\n","Z, X = generate_HMM(P, pi, E, T=T)\n","\n","fn='obs_group1.txt'  # choose a group-specific filename\n","\n","# write ######\n","with open(fn, 'w') as f:\n","  m = map(str, X)            # convert numbers to strings\n","  f.write(' '.join(list(m))) \n","\n","# read (for later) ######\n","with open(fn, \"r\") as f:\n","  line  = f.readline().split()  # read first line and split\n","Xr = list(map(np.int64, line))  # map line to np.int64\n","\n","print('X =',*X)\n","print('Xr=',*Xr)\n","np.all(X==Xr)"]},{"cell_type":"markdown","metadata":{"id":"Cq0uHXT6nWKG"},"source":["# Applications: What is this all good for?"]},{"cell_type":"markdown","metadata":{"id":"X7IOq7c3nZSe"},"source":["- Speech Recognition\n","- DNA sequence analysis: sequence segmentation\n","- Summarizing Multiple Alignments: profile HMM\n","- Robot Location\n"]},{"cell_type":"markdown","metadata":{"id":"rD7kUxU4rKho"},"source":["# Reference: Conventions and Notations"]},{"cell_type":"markdown","metadata":{"id":"CLO6ncMQrRYj"},"source":["- Number of States: $i=1,2, \\ldots N$\n","- Number of Observations (discrete values): $k=1,2, \\ldots M$\n","- Length of Sequence: $t=1,2, \\ldots T$\n","- conditional probabilities (notice the index order !)\n","  - $P_{ij} = Pr(Z_{t+1}=j|Z_t=i)$ \n","  - $E_{ik} = Pr(X_{t}=k|Z_t=i)$ \n","- Transition Matrix: $P=(P_{ij})$ $\\longrightarrow N \\times N$ matrix\n","- Emission Matrix: $E=(E_{ik})$ $\\longrightarrow N \\times M$ matrix\n","- Initial Probability: $\\pi_i$  $\\longrightarrow N$ dim. row vector\n","- Sequences: condensed notation\n","  - observations $X = X_{1:T} = (X_1, X_2, \\ldots, X_T)$\n","  - hidden states $Z = Z_{1:T} = (Z_1, Z_2, \\ldots, Z_T)$\n","\n","\n","**Notice:** \n","\n","- Both states and observations are discrete variables (e.g. $Z=GGGGIIIIGGGG...$ and $X=ACTGTCGCGCGATTA$) but they are often encoded as integer variables, e.g. $Z=000011110000$\n","- more condensed notation: $Pr(Z_t=i) = Pr(Z_t)$\n","- all $X_t$ are observed, so $E_{ik}$ serves as a look-up table. Sometimes I write $E_{it}$ \n","- Python indices of arrays start at 0"]},{"cell_type":"markdown","metadata":{"id":"m6zZIR6xfSi-"},"source":["# Joint Probability: $Pr(X,Z)$"]},{"cell_type":"markdown","metadata":{"id":"g13M5yh-XNSE"},"source":["Hidden Markov Models are graphical models:\n","\n","\n","<div>\n","   <img src=\"https://github.com/thomasmanke/ABS/raw/main/figures/HMM_Joint.jpg\",  width=\"1000\">\n","</div>\n"]},{"cell_type":"markdown","metadata":{"id":"H8QN7xLZfd0l"},"source":["$$ \n","Pr(X,Z) = Pr(X|Z) Pr(Z) = Pr(Z_1) Pr(X_1|Z_1) \\prod_{t=2}^T Pr(Z_t | Z_{t-1}) Pr(X_{t}|Z_t)\n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"MbD4MwHTXVnK"},"source":["**Message**: \n","- Chain together probabilities of initial state, state transitions and observed emissions!\n","- For given sequences ($X$ and $Z$), multiply all edge probabilities in graphical model ! \n","- Recursion Principle: if partial solution $Pr(X_{1:t}, Z_{1:t})$ is available,  $Pr(X_{1:t+1}, Z_{1:t+1})$ can be obtained iteratively"]},{"cell_type":"markdown","source":["**Task (10 min)**: \n","\n","Given the HMM parameters and an observed sequence $X=002$. Calculate the probability of all possible paths \n","- Group 1: all paths starting with $G$\n","- Group 2: all paths starting with $S$\n","Report back the path with the highest probability and compare"],"metadata":{"id":"XVK9rPUBEF_r"}},{"cell_type":"code","source":["path000 = pi[0]*E[0,0]*P[0,0]*E[0,0]*P[0,0]*E[0,2]  # path GGG\n","path001 = "],"metadata":{"id":"Br2gG6oDFCWh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bT6xv74T87fi"},"source":["**Notice:** \n","- Condensed notation, e.g. $Pr(X,Z) = Pr(X_1=x_1, X_2=x_2, \\ldots, Z_1=z_1, Z_2=z_2, \\ldots, Z_T=z_T)$ \n","- $\\sum_X \\sum_Z P(X,Z) = 1$\n","- for discrete Markov chains both $X_t$ and $Z_t$ are discrete variables. Their values are often represented by integers (e.g. X=001123001)\n","- Strictly we may want to write $Pr(X,Z|\\Theta)$ to highlight the conditioning on known parameters $\\Theta=(P,E,\\pi)$. Unless stated otherwise, we assume that those parameters are all known and fixed."]},{"cell_type":"markdown","metadata":{"id":"xlvj-Feu8XJV"},"source":["**Discussion (5 min):** \n","\n","- How many possible sequences are there for a) observations $X$ and b) hidden states $Z$?\n","- What happens to $Pr(X,Z)$ if the sequence $Z$ contains forbidden transitions?\n","- Why would one want to know the joint?\n","- Why can we not calculate it ?"]},{"cell_type":"markdown","source":["# Typical Problems"],"metadata":{"id":"EUS7uJgwbK4o"}},{"cell_type":"markdown","metadata":{"id":"Q5pQh27N4vz6"},"source":["\n","\n","- given model parameters + observations $X$ \n","  - scoring (observations): $Pr(X) \\longrightarrow$ **Forward Algorithm**\n","  - decoding (hidden variables):\n","    - likelihood $Pr(Z_t=i|X) \\longrightarrow$  **Forward-Backward Algorithm**\n","    - best state sequence: $argmax_Z Pr(Z|X) \\longrightarrow$ **Viterbi Algorithm**\n","\n","- given only observations $X$:\n","  - learning (model parameters)  $\\longrightarrow$ **Baum-Welch Algorithm**"]},{"cell_type":"markdown","metadata":{"id":"0mM0HRdYWLEJ"},"source":["# Probability of Observations: $Pr(X)$"]},{"cell_type":"markdown","metadata":{"id":"12MOiyxIX5vd"},"source":["**Uses:** Evaluate (score) observations. Compare different models: $P(X|\\Theta_1)$ vs $P(X|\\Theta_2)$"]},{"cell_type":"markdown","metadata":{"id":"b0o6LekPECm-"},"source":["**First idea: Chain rule**\n","$$\n","P(X) = P(X_1) P(X_1|X_2) P(X_3|X_1,X_2) \\ldots P(X_T| X_1, \\ldots ,X_{T-1})\n","$$\n","\n","... not calcuable from HMM parameters"]},{"cell_type":"markdown","metadata":{"id":"XUYRpDW7DR9U"},"source":["**Second idea: Naive Marginalization**\n","\n","Use joint distribution $Pr(X,Z)$ and remove hidden state sequence (it is unobservable) $\\to$ marginalize\n","\n","$$\n","Pr(X) = \\sum_Z P(X,Z) = \\sum_Z Pr(X|Z) Pr(Z)\n","$$"]},{"cell_type":"markdown","metadata":{"id":"XMIigFgFX34E"},"source":["**Notice:**\n","- remember: each term in sum breaks into emission probabilities, transition probabilities (and initial state probability)\n","- marginalization over *all possible* state paths $Z$ ($=Z_{1:T} = Z_1 Z_2 \\cdots Z_T$) \n","- $N^T$ paths for $N$ possible states and sequences of length $T$ -- unfeasible\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3FUsE_8LWXfb"},"source":["**Third idea: Dynamic Programming (reuse previous calculations)**"]},{"cell_type":"markdown","source":["### The Forward Algorithm"],"metadata":{"id":"I5UbfOn3T1RA"}},{"cell_type":"markdown","source":["We don't know any $Z_t$, so we need to **track all possibilities**: Trellis graph."],"metadata":{"id":"Dx1Ny7tQcr_y"}},{"cell_type":"markdown","metadata":{"id":"1PutwYFAZDaf"},"source":["<div>\n","   <img src=\"https://github.com/thomasmanke/ABS/raw/main/figures/HMM_HiddenTrellis.jpg\",  width=\"1000\">\n","</div>\n","\n","Let's assume that at some time $t$ we already know the joint probability for the observed sequence $X_{1:t}$ and the hidden state $Z_t$ (for each possible value of $Z_t=i$).\n","\n","This information is stored in the **forward variable:** $\\alpha_{ti} = Pr(X_{1:t}, Z_t=i)$. This is a vector of joint probabilities that will be propagated forward in time.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"L6ak8wLaaxKJ"},"source":["#### Iteration"]},{"cell_type":"markdown","metadata":{"id":"h6Vlsnm3a7q-"},"source":["- **1. Initialization** ($t=1$): \n","$$\n","\\alpha_{1i} = Pr(X_1, Z_1=i) = Pr(X_1|Z_1=i) Pr(Z_1=i)\n","$$\n","  - \n","   $\\to$ element-wise multiplication of a row from emission matrix with initial state distributions\n","  - $Pr(X_1|Z_1=i)$ is one element of the emission matrix $E_{ik}$ (row i = state, column k = k(1) observed valuee of $X_1$). \n","  - $Pr(Z_1=i)$ is the i-th element of the initial state distribution $\\pi_i$.\n","\n","\n","\n","- **2. Induction ($t \\to t+1$):** state transition + new observation \n","  - *2.1 state transition ($Z_t \\to Z_{t+1}$):* Consider all possible Markov transitions and sum them up.\n","$$\n","\\begin{align}\n","Pr(Z_{t+1}=i, X_{1:t}) &= \\sum_k Pr(Z_{t+1}=i, Z_t=k, X_{1:t}) \\\\\n","&=\\sum_k Pr(Z_{t+1}=i|Z_t=k, X_{1:t}) Pr(Z_t=k,X_{1:t}) \\\\\n"," &= \\sum_{k} P_{ki} \\alpha_{tk}  = \\sum_{k} \\alpha_{tk} P_{ki}\n","\\end{align}\n","$$\n","$\\to$ matrix multiplcation of row vector $\\alpha_{tk}$ with transition matrix\n","  - *2.2 new observation ($X_{t+1}$):* consider emission probability resulting in observation $X_{t+1}$\n","$$\n","Pr(Z_{t+1}=i, X_{1:t}, X_{t+1}) = Pr(X_{t+1}|Z_{t+1}=i) Pr(Z_{t+1}=i, X_{1:t})\n","$$\n","$\\to$ element-wise multiplication of a row from emission matrix with \n","\n","2 Steps (graphical interpretation)\n","<div>\n","   <img src=\"https://github.com/thomasmanke/ABS/raw/main/figures/HMM_Forward.jpg\",  width=\"800\">\n","</div>\n","\n","- **3. Termination ($t=T$):**\n","$$\n","Pr(X) = Pr(X_{1:T}) = \\sum_i \\alpha_{Ti}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"yA-z4Jg2i_Hz"},"source":["**Notice:**\n","- marginalization: $Pr(X_{1:t}) = \\sum_i \\alpha_{ti} \\ne 1$.  In fact, it is much smaller than 1 for large $t$ !\n","- Propagation of $\\alpha_{tk}$ is linear in sequence length $T$\n","- Calculation of $Pr(X)$ requires $T N^2$ calculations $\\ll N^T$ \n","- Example: $(N,T) = (2, 100) \\longrightarrow 400 \\ll 2^{100}$  \n","- Emission matrix $E_{ik}$ serves as lookup table for given observation $X_t=k$ at time $t$. ($k=f(t)$)"]},{"cell_type":"markdown","source":["### Summary: forward recursion is fast\n","<div>\n","   <img src=\"https://github.com/thomasmanke/ABS/raw/main/figures/HMM_Forward_summary.jpg\",  width=\"800\">\n","</div>\n"],"metadata":{"id":"BO4gayKdSbF2"}},{"cell_type":"markdown","metadata":{"id":"_BSfxE-iT1Zb"},"source":["### Group Task (20 min)"]},{"cell_type":"markdown","metadata":{"id":"XvwDurm2UH82"},"source":["Given the above HMM  with 2 states (Germany=0, Switzerland=1) and a magically known joint probability $Pr(Z_{t-1}, X_{1:t-1})=(0.05, 0.02)$.\n","This denotes the probability for the two states **and** all observations until time $t-1$. (Notice that this does not have to sum to 1!)\n","\n","Calculate the updated probability for $Z_t=$ Germany (0) **and** that the newly observed emission is Bread (0) or Fondue (2). \n","\n","- Group 1: $P(Z_t=0, X_t=0) = $ ? \n","- Group 2: $P(Z_t=0, X_t=2) = $ ?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7VxyLBGnUH82"},"outputs":[],"source":["import numpy as np\n","\n","alpha = np.array([0.05, 0.02]) # initial probability   \n","\n","alpha = ...     # state transition\n","print('after state transition: ', alpha)\n","\n","x = ...                          # define observation at time t\n","LH=E[ ... ]                      # get all emission that result in state x\n","print('emission vector:        ', LH)\n","\n","alpha = ...                # take into account observations\n","print('new probability         ', alpha) \n","\n","# for calculation of conditional probability\n","alpha /= np.sum(alpha)         # normalized posterior\n","print('posterior norm:', alpha)"]},{"cell_type":"markdown","source":["**More remarks:**\n","- We assumed that all parameters of the HMM are known: $\\Theta=(P,E,\\pi)$\n","\n","\n","- *Monitoring:* The above derivation could also have been done for the conditional probability: $Pr(Z_t=i|X_{t})$. The only difference is that new observations are incorporated using Bayes theorem\n","$$\n","Pr(Z_{t+1}=i, X_{1:t+1}) = Pr(X_{t+1}|Z_{t+1}=i) Pr(Z_{t+1}=i, X_{1:t})\n","$$\n","$$\n","Pr(Z_{t+1}=i | X_{1:t+1}) \\propto Pr(X_{t+1}|Z_{t+1}=i) Pr(Z_{t+1}=i | X_{1:t})\n","$$ \\\\\n","Here $P(Z_{t+1}=i|X_{1:t})$ will still have to be normalized at each time $t$ such that $\\sum_k P(Z_{t+1}=k|X_{1:t}) = 1$\n","\n"],"metadata":{"id":"HrR31S0CTEuK"}},{"cell_type":"markdown","source":["**Follow up task (5 min)**\n","\n","Rather than calculating the joint probabilities, adjust the above cell to calculate the more interesting conditional probablities:\n","\n","- Group 1: $P(Z_t=0 | X_t=0) = $ ? \n","- Group 2: $P(Z_t=0 | X_t=2) = $ ?\n","\n","Assume that the conditional probability at time $(t-1)$ is $Pr(Z_{t-1}=i| X_{1:t-1})=(0.75, 0.25)$. (This has to sum to 1).\n","\n","Hint: Apart from the the obvious change for the initial alpha, this needs only one additional line of code."],"metadata":{"id":"2-vYXqPjgHe4"}},{"cell_type":"code","source":[""],"metadata":{"id":"GX2QnTU4KH_H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Calculating $P(Z_t=i| X_{1:t}$ for all $t$ amounts to simple (forward) loop over all times; it is linear in $t$."],"metadata":{"id":"Qqzfvvc6Kino"}}],"metadata":{"colab":{"collapsed_sections":["rD7kUxU4rKho"],"name":"HMM_000.ipynb","provenance":[],"authorship_tag":"ABX9TyMoIxKP3naJhzoihAbnPqic"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
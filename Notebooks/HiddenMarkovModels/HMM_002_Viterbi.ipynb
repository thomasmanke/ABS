{"cells":[{"cell_type":"markdown","metadata":{"id":"c0MIrg0YkBeg"},"source":["# Status Quo: Where are we ?"]},{"cell_type":"markdown","metadata":{"id":"5MK7wdRRkHL9"},"source":["We have an HMM model and parameters (emissions, transitions and initial probability).\n","\n","Remember the travel analogy: we used countries (as hidden states) and food (as observables) - but we could have chosen many other stories."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QfQwDUSJ5-fI"},"outputs":[],"source":["import numpy as np\n","pi=np.array( [0.75, 0.25] )                          # initial state probability\n","P =np.array([ [0.8, 0.2], [0.1, 0.9] ])              # transition probabilites\n","E =np.array([ [0.7, 0.2, 0.1], [0.1, 0.1, 0.8] ])    # emission probabilities"]},{"cell_type":"markdown","metadata":{"id":"aYH5l5q4jTl1"},"source":["With these parameters we can generate state sequences $Z$ and observations $X$, but in typical applications the state sequences $Z$ are unobserved. We should think about $Z$ probabilistically. Until now we have encountered the following distributions:\n","\n","- \"the joint\" $P(X,Z)$: nice theoretical form (graphical model), but in general not calculable ($Z$ unknown)\n","- \"scoring\" $P(X)$: how to sum over all $Z$ efficiently $\\to$ **Forward algorithm**\n","- \"monitoring\" $P(Z_t=i|X_{1:t})$: similar to $P(X)$ but with extra normalization (Bayesian update)\n","\n","Naively, these probabilites are very expensive to calculate $\\to$ use efficient **recursion** (thanks to the Markov property)."]},{"cell_type":"markdown","metadata":{"id":"lj-s9Y2fkqMg"},"source":["Remember our remaining goals:\n","\n","- $P(Z_t=i| X)$ what are the state probabilities at $t$ given **all** observations $X=X_{1:T}$ (past, present and future)\n","- $argmax_Z P(Z|X)$ what is the most likely path $Z$\n","- $argmax_\\Theta P(\\Theta|X)$ what are the best parameters "]},{"cell_type":"markdown","metadata":{"id":"06CWZ-REUbP8"},"source":["# $P(Z_t | X_{1:T})$ : Hindsight (Forward-Backward)"]},{"cell_type":"markdown","metadata":{"id":"7j90GlKLnZ__"},"source":["## The Algorithm"]},{"cell_type":"markdown","metadata":{"id":"qR7pCzzMLSg_"},"source":["Now we want to calculate the state probability at time $t$ given **all** observations $X_{1:T}$. This is also called *smoothing* or posterior decoding."]},{"cell_type":"markdown","metadata":{"id":"i0Rbh4JZEqNx"},"source":["$$\n","\\gamma_{ti} = P(Z_t=i|X_{1:T}) = Pr(Z_t=i| X_{1:t} X_{t+1:T}) \\propto  Pr(Z_t=i|X_{1:t}) \\cdot Pr(X_{t+1:T}|Z_t=i)\n","$$"]},{"cell_type":"markdown","metadata":{"id":"-qmXjBwM9lqR"},"source":["The first factor is $\\alpha_{ti}$ from the **Forward Algorithm**. The second factor denotes the likelihood of future observations, given $Z_t$. \n","\n","It can be calculated analoguously by the **Backward Algorithm**\n","\n","$$\n","\\begin{align}\n","\\beta_{ti} &= Pr( X_{t+1:T} | Z_t=i) \\\\\n","&= \\sum_k Pr(X_{t+2:T}|Z_{t+1}=k) Pr(X_{t+1} | Z_{t+1}=k) Pr(Z_{t+1=}k|Z_t=i) \\\\\n","&= \\sum_k \\beta_{t+1 k} Pr(X_{t+1}|Z_{t+1}=k) Pr(Z_{t+1=}k|Z_t=i)\n","\\end{align}\n","$$\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gKxxTMyRwtkx"},"source":["$$\n","\\mbox{Initalisation (t=T):}~~~ \\beta_{Ti} = 1~~~~~~~~\n","\\mbox{Termination (t=1):}~~~\\sum_i \\beta_{1i} = Pr(X) \\\\\n","$$"]},{"cell_type":"markdown","metadata":{"id":"2JSkqQ9pnkPC"},"source":["## Summary"]},{"cell_type":"markdown","metadata":{"id":"0UQaEHSKwJcn"},"source":["<div>\n","   <img src=\"https://github.com/thomasmanke/ABS/raw/main/figures/HMM_ForwardBackward.jpg\",  width=\"1200\">\n","</div>\n","\n","Ultimately this leads to **Forward-Backward Algorithm** (+ Normalization)\n","\n","$$\n","Pr(Z_t = i | X_{1:T}) = \\gamma_{ti} = \\frac{\\alpha_{ti} \\beta_{ti}}{\\sum_k \\alpha_{tk} \\beta_{tk}} \n","$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GtuIp45lnrK-"},"source":["## Uses"]},{"cell_type":"markdown","metadata":{"id":"1rE9oTnfnm56"},"source":["- assign a posterior state probabilities to each time  \n","- sample $Z_t|X$ beyond just the best $Z_t$\n","- parameter estimates: couple with EM (Baum-Welch)"]},{"cell_type":"markdown","metadata":{"id":"4vX9p0TKV5dH"},"source":["# The good news: There is software"]},{"cell_type":"markdown","metadata":{"id":"fMqkflalV_MO"},"source":["Bad news: there are many different packages. Here we use hmmlearn."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vujmTrALZdQD"},"outputs":[],"source":["#%%script echo install only once\n","!pip install hmmlearn"]},{"cell_type":"markdown","metadata":{"id":"Wun9E8qPZYd-"},"source":["## HMMlearn: Generating Sequences and Observations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CfZCAgZRZr6h"},"outputs":[],"source":["import numpy as np\n","from hmmlearn import hmm\n","\n","pi=np.array( [0.75, 0.25] )                          # initial state probability\n","P =np.array([ [0.8, 0.2], [0.1, 0.9] ])              # transition probabilites\n","E =np.array([ [0.7, 0.2, 0.1], [0.1, 0.1, 0.8] ])    # emission probabilities\n","\n","np.random.seed(42)                       # only for reproducibiltiy\n","\n","# define HMM and set parameters\n","model = hmm.MultinomialHMM(n_components=2)\n","model.startprob_ = pi                    # initial state prob\n","model.transmat_  = P                     # transition prob\n","model.emissionprob_ = E                  # emission prob\n","\n","# generate sequence\n","X,Z = model.sample(50)                   # c.f. Z, X = generate_HMM(P,pi,E)\n","print('states Z       =',*Z.flatten())\n","print('observations X =',*X.flatten())"]},{"cell_type":"markdown","metadata":{"id":"dkQPv3NExEeE"},"source":["## HMMlearn: Calculating $Pr(X)$"]},{"cell_type":"markdown","metadata":{"id":"3cTn01VMxKTA"},"source":["Remember that the Forward Algorithm is used to circumvent the difficult calculation\n","\n","$$\n","Pr(X) = \\sum_Z Pr(X,Z)\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BZaMl1iOxbjf"},"outputs":[],"source":["score = model.score(X) \n","print('score Pr(X) = ', score)"]},{"cell_type":"markdown","metadata":{"id":"mevy0ke4yWQo"},"source":["**Discussion**: What's the meaning of this score? Consult the help pages to find out more about this score."]},{"cell_type":"markdown","metadata":{"id":"cx_cKRH7yXV7"},"source":["## HMMlearn: Calculating $Pr(Z_t=i | X)$ "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Ua5BItnbyUT"},"outputs":[],"source":["PrZ = model.predict_proba(X) "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oniQxQCDcDOo"},"outputs":[],"source":["# use Pandas for dataframes and powerful plotting\n","import pandas as pd\n","df=pd.DataFrame(data=PrZ,index=Z)  # convert PrZ to dataframe and add true Z as index\n","pp=df.plot(kind='bar', stacked=True, figsize=(20,5), title='P(Z_t|X)')"]},{"cell_type":"markdown","metadata":{"id":"EdV0yR3EhITA"},"source":["This is how the posterior probability of $Z_t =i$ changes over time. \n","\n","**Individual task (15 min)**: Take some time and try to understand the results and data structures above.\n","Does the plot make sense? If this shows the \"posterior\" state probability; what is the \"prior\" state probability and how could you obtain it?"]},{"cell_type":"markdown","metadata":{"id":"jYzyTCWenJ2U"},"source":["# Decoding: Finding the Best State Sequence"]},{"cell_type":"markdown","metadata":{"id":"s2B9t_-8UqFl"},"source":["## Option 1: Maximal Posterior"]},{"cell_type":"markdown","metadata":{"id":"KYQoK3jLOQ0j"},"source":["The above calculation $Pr(Z_t=i|X_{1:T})$ suggests a simple way to define the \"best\" state sequence. \n","\n","For example, one might simply collect the state sequence with the largest $Pr(Z_t=i|X)$ for each time $t$:\n","\n","$$\n","Z_t = \\underset{i}{argmax} Pr(Z_t=i | X)\n","$$\n","\n","**Discussion**: \n","\n","Refer back to the above figure for $Pr(Z_t=i|X)$ and see whether this is a good prescription (remember that in this case we know the true $Z$). What could be possible problems with it?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7yx6hrGXqUpY"},"source":["## Option 2: Viterbi Decoding"]},{"cell_type":"markdown","metadata":{"id":"DaUtms9rqDMF"},"source":["Using the maximal posterior (as above) may lead to forbidden state transition $Z_t \\to Z_{t+1}$, where the corresponding transition probability is 0.\n","\n","For this reason, we are also looking for a **valid path** $Z_{1:T}$ that maximizes the conditional probability given all observations $X_{1:T}$."]},{"cell_type":"markdown","metadata":{"id":"FQErxEYhnQBW"},"source":["$$\n","Z^\\ast = \\underset{Z}{argmax} Pr(Z|X) \\propto \\underset{Z}{argmax} Pr(Z,X) \\\\\n","$$"]},{"cell_type":"markdown","metadata":{"id":"PL8xork1rcLz"},"source":["### Recursion: again"]},{"cell_type":"markdown","metadata":{"id":"JY5xiQc5Rufr"},"source":["Again we will utilize the **recursion principle** - this time assuming that for a given time $t$ we know\n","- the path with maximum probability that ends in $Z_t=i$\n","- the last link of that path that connects  to $Z_t=i$, i.e. it's origin.\n","\n","**Goal:** maximize over all previous\n","\n","$$\n","\\mu_{ti} = \\max_{Z_{1:t-1}} Pr(Z_t=i, Z_{1:t-1}, X_{1:t})\n","$$\n","\n","For time $t+1$ we can use the usual propagation of probabilities.\n","\n","<div>\n","   <img src=\"https://github.com/thomasmanke/ABS/raw/main/figures/HMM_Viterbi.jpg\",  width=\"1500\">\n","</div>\n","\n","Notice:\n","\n","- This is the Forward Algorithm with $\\sum_k \\to \\max_k$\n","- $\\max_i \\mu_{Ti} = \\max_{Z} Pr(Z,X)$\n","- forbidden paths would have some transitions $Pr(Z_{t+1}=j | Z_{t}=i) = 0 \\longrightarrow$ they would never be maximal.\n","- complexity: $O(T) \\ll O(N^T)$\n","- also need to keep track of last links $\\lambda_{ti}$ (the maximizing transitions) $\\to$ back-tracking\n"]},{"cell_type":"markdown","metadata":{"id":"PB5i-tjf0wb5"},"source":["## HMMlearn: Decoding\n","\n","The hmmlearn implementation is very easy to use"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8A4Icfz0EEKJ"},"outputs":[],"source":["_, Zv = model.decode(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aZQ-V1-t1BSc"},"outputs":[],"source":["%%script echo This would be an alternative approach (also with hmmlearn)\n","# alternatives\n","PrZ = model.predict_proba(X)    # maximum posterior probability \n","Zm = PrZ.argmax(axis=1)         # get maximum\n","Zv = model.predict(X)            # Viterbi pathh"]},{"cell_type":"markdown","metadata":{"id":"YfxY5UrS2rdt"},"source":["**Group Task (30 min)**: \n","\n","1. Sample the HMM model again with T=50\n","2. Consult the help for model.decode() and identify two different methods to predict a hidden state sequence for a given X.\n","3. Try both algorithms and look for differences in the prediced sequences.\n","4. Compare the predicted state sequence with the \"hidden\" sequence Z\n","5. Increase the generated sequence length to 1000 and repeat the comparisons."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"arhyudT0BA1q"},"outputs":[],"source":["%%script echo edit before execution\n","np.random.seed(42)\n","T=1000\n","X,Z = ... sample T timesteps from model ...\n","\n","# get two different \"best\" paths from two methods \n","[ model.decode( ... method 1 ...) ]\n","[ model.decode( ... method 2 ...) ]\n","\n","# Check differences\n","print('differences (Z1-Z):  ', np.sum(Z1 != Z))\n","print('differences (Z2-Z):  ', np.sum(Z2 != Z))"]},{"cell_type":"markdown","metadata":{"id":"ILYU7fOqPX8o"},"source":["# HMMLearn: Fitting"]},{"cell_type":"markdown","metadata":{"id":"P6ERzCcoXNWP"},"source":["This is by far the comoutationally most demanding task"]},{"cell_type":"markdown","metadata":{"id":"qsgXkRwkcM2r"},"source":["In an *observable* Markov model, the transition probabilities could be estimated from the observed transition frequencies $(Z_t=i \\to Z_{t+1}=j)$\n","$$\n","\\hat{P}_{ij} = \\frac{N_{ij}}{\\sum_l N_{il}}\n","$$\n","\n","If $Z$ was observable, one could define similar estimates for the emission probabilities $\\hat E_{ik}$ from observed emission frequencies $(Z_t=i \\to X_{t}=k)$\n","However, the hidden states $Z$ are **not** observable.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"AtiqAUOY_La2"},"source":["## If Z was observable (skip)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HOr2P8bffd0I"},"outputs":[],"source":["L=len(Z)    # length of observed sequence\n","\n","trans={}\n","for i in range(L-1):\n","  pair = '{}->{}'.format(Z[i],Z[i+1])\n","  trans[pair] = trans.get(pair, 0) + 1\n","\n","emiss={}\n","for i in range(L):\n","  pair = '{}->{}'.format(Z[i],X[i])\n","  emiss[pair] = emiss.get(pair, 0) + 1\n","\n","\n","print('transitions:')\n","for k, v in trans.items():\n","  print(k,v)\n","\n","print('emissions:')\n","for k, v in emiss.items():\n","  print(k,v)\n"]},{"cell_type":"markdown","metadata":{"id":"uTDanwCIqj0y"},"source":["**Discussion:** Why would this not necessarily be a good estimate? (Longer chains vs multiple chains)"]},{"cell_type":"markdown","metadata":{"id":"toUXxR0f3yBf"},"source":["## If Z is hidden (Baum-Welch) "]},{"cell_type":"markdown","metadata":{"id":"7kqyLaG5TRPz"},"source":["1. Initialize all probabilities randomly\n","2. Re-estimation: change parameters iteratively to maximize $Pr(X)$\n","\n","The re-estimation procedure makes clever use of Forward and Backward algorithm, but one could also use gradient techniques for optimization.\n","\n","The iteration will converge to **local maxima** and will generally depend on the initial conditions, and the number of iterations.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O_mYhPYwSuQC"},"outputs":[],"source":["model_fit = hmm.MultinomialHMM(n_components=2)\n","model_fit.fit(X)\n","score = model_fit.score(X)"]}],"metadata":{"colab":{"collapsed_sections":["06CWZ-REUbP8","7j90GlKLnZ__","2JSkqQ9pnkPC","GtuIp45lnrK-","s2B9t_-8UqFl","7yx6hrGXqUpY","PL8xork1rcLz","AtiqAUOY_La2"],"provenance":[]},"kernelspec":{"display_name":"Python 3.10.7 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}

{"cells":[{"cell_type":"markdown","metadata":{"id":"howMdVCwrP1D"},"source":["# Motivation and Simulation"]},{"cell_type":"markdown","metadata":{"id":"NKNxmVkcrS1q"},"source":["Even if the \"states of the world\" are Markovian, they are often hidden from us, and we only observe some measurements. \n","\n","**A traveling analogy**\n","\n","> I frequently commute between two states: Germany and Switzerland. Let's assume my travels can be modelled as a Markov Process, as described in the previous section. But now I only communicate my dinner plans with the world. Therefore dinner is an **observable** variable, but my current state (the country) variable is **hidden.** We might hope that something could still be learned about the states visited from the observation on food consumption.\n","\n","<div>\n","   <img src=\"https://github.com/thomasmanke/ABS/raw/main/figures/HMM_CountryFood.jpg\",  width=\"1000\">\n","</div>\n","\n","\n","This is a Hidden Markov Model (HMM). An HMM is characterized by three ingredients:\n","\n","- initial distribution: $P(Z_0=i)=\\pi_i$ ( $\\to 1 x N$ matrix = row vector )\n","- transition matrix: $P(Z_t=j|Z_{t-1}=i) = P_{ij}$  ( $\\to N \\times N$ matrix )\n","- emission matrix: $P(X_t=k|Z_t=i) = E_{ik}$ ( $ \\to N \\times M$ matrix )\n","\n","The emission probabilities are dependent on the state, but constant over time.\n","\n","For simplicity we will assume that both states and observables are discrete.\n","To be specific, the Hidden Markov Model with 2 states $Z \\in$ {Germany=0, Switzerland=1} and observations with 3 possible observations $X \\in$ {Bread=0, Fish=1, Fondue=2} may read:\n","\n","$$\n","\\begin{align}\n","    P(Z_0) &= \\begin{bmatrix} 0.75 & 0.25  \\end{bmatrix} \\\\ \\\\\n","    P(Z_t | Z_{t-1}) & = \\begin{bmatrix} 0.8 & 0.2 \\\\ 0.1 & 0.9 \\end{bmatrix} \\\\ \\\\\n","    P(X_t | Z_t) & =  \\begin{bmatrix} 0.7 & 0.2 &  0.1 \\\\ 0.1 & 0.1 & 0.8 \\end{bmatrix} \\\\\n","\\end{align}\n","$$\n","\n","**Discussion:** Give an interpretation of the numbers as they relate to the graph above.  \n","\n","**Notice:** all rows are non-negative and they sum to 1 (*stochastic* matrices)\n","\n","The cell below specifies all these parameters in Python/Numpy.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QfQwDUSJ5-fI"},"outputs":[],"source":["import numpy as np\n","pi=np.array( [0.75, 0.25] )                          # initial state probability\n","P =np.array([ [0.8, 0.2], [0.1, 0.9] ])              # transition probabilites\n","E =np.array([ [0.7, 0.2, 0.1], [0.1, 0.1, 0.8] ])    # emission probabilities"]},{"cell_type":"markdown","metadata":{"id":"q4AcKuQ-D64O"},"source":["**Group Task (30 min):** Discuss and simulate the above Hidden Markov Model. \n","\n","Complete the following function and generate observations from a Hidden Markov Model defined above. You might want to refer back to the first lecture on simple Markov Models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KLBgT-663Xcp"},"outputs":[],"source":["# notice the similarities with generate_sequence() from the plain Markov Model\n","def generate_HMM(P, pi, E, T=50):\n","  assert P.shape[0]==P.shape[1],         \"generate_HMM: P should be a squared matrix\"\n","  assert E.shape[0]==P.shape[0],         \"generate_HMM: E and P should have the same number of rows (states)\"\n","  assert np.allclose( P.sum(axis=1), 1), \"generate_HMM: P should be a stochastic matrix\"\n","  assert np.allclose( E.sum(axis=1), 1), \"generate_HMM: E should be a stochastic matrix\"\n","  assert np.isclose( pi.sum(), 1),       \"generate_HMM: pi should sum to 1\"\n","  \n","  \n","  # first define two list (states = integers, emissions = letters)\n","  ns = ...                          # number of states\n","  ne = ...                          # number of outputs (#observables)\n","  states= list(range(ns))           # state labels as integers\n","  emissions=list(range(ne))         # observation labels as integers\n","\n","  # chose first state and emission\n","  z = np.random.choice( states,    p = ... )\n","  x = np.random.choice( emissions, p = ... )\n","\n","  # add state and observation to history\n","  state_hist = [z]\n","  emit_hist = [x]\n","  \n","  # loop for T time steps\n","  for t in range(T):\n","    z = np.random.choice( states,    p = ... )\n","    x = np.random.choice( emissions, p = ... )\n","\n","    # collect history with state and emission labels\n","    state_hist.append(z)\n","    emit_hist.append(x)\n","  return state_hist, emit_hist"]},{"cell_type":"markdown","metadata":{"id":"WMzW0gd_OMfC"},"source":["**Test:** If done correctly, the function should return output such as"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":293,"status":"ok","timestamp":1654957716939,"user":{"displayName":"Thomas Manke","userId":"17591636328965298454"},"user_tz":-120},"id":"8B8dzc_y-HcV","outputId":"c093d5fe-661c-4798-fc6e-e309ed4d6278"},"outputs":[{"name":"stdout","output_type":"stream","text":["states Z       = 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0\n","observations X = 2 0 0 1 1 2 2 1 2 2 1 2 2 2 0 1 2 2 0 2 2 2 0 0 0 2 2 2 0 0 0 2 2 2 2 0 1 1 0 0 2 0 2 2 2 2 2 2 2 0 0\n"]}],"source":["%%script echo ensure that function generate_HMM() is defined\n","np.random.seed(42)\n","Z, X = generate_HMM(P,pi,E, T=50)\n","print('states Z       =',*Z)\n","print('observations X =',*X)"]},{"cell_type":"markdown","metadata":{"id":"1ztb1K8jl2Y8"},"source":["**Discussion:** Do these sequences make sense? Can you give an  interpretation of the observation?"]},{"cell_type":"markdown","metadata":{"id":"fSZYk78UAFcs"},"source":["# Interlude: Joints, Marginals, Conditionals, Bayes & All That"]},{"cell_type":"markdown","metadata":{"id":"kGCMossql2Y9"},"source":["This interlude will apply beyond Hidden Markov Models, but I will use the above emission probablities as an example.\n","\n","So let's be discrete: with two variables: $Z \\in \\{0,1\\}$ and $X \\in \\{0,1,2\\}$. \n","\n","Much of what follows applies also to continuous variables, where discrete distributions can be replaced by probability density functions, sums by integrals - and the usual mathematical concerns about the existence of limits and such.\n"]},{"cell_type":"markdown","source":["## Joint & Co"],"metadata":{"id":"O0TFgc0e-dEw"}},{"cell_type":"markdown","source":["\n","<div>\n","   <img src=\"https://github.com/thomasmanke/ABS/raw/main/figures/JointConditionalMarginal.jpg\",  width=\"1000\">\n","</div>"],"metadata":{"id":"OmtNP4ok7VW2"}},{"cell_type":"markdown","metadata":{"id":"1SLYvERal2Y9"},"source":["Knowing the **joint distribution** $P(X,Z)$ is the best we can hope for, since everything else can (in principle) be calculated from it.\n","\n","However:\n","\n","a) remember that we are about to hide all state variables $Z$.\n","\n","b) calculations maybe very hard - analytically and computationally. For example, even if we knew a joint distribtuions such as \n","\n","$$\n","P(X_1, X_2, \\ldots, X_T, Z_1, \\ldots Z_T)\n","$$\n","\n","many computational task would become very difficult (combinatorics!) - unless the problem has some structure (such as a Markov Property).\n","\n","\n","\n"]},{"cell_type":"markdown","source":["In many situations we may be more interested in specific subsets of variables: \n","\n","- **conditional distributions**: some variables are known or fixed, \n","- **marginal distributions** some variables are uninteresting $\\longrightarrow$ average over."],"metadata":{"id":"oLHiEHMD9Zqi"}},{"cell_type":"markdown","source":["## Bayes Theorem"],"metadata":{"id":"2q91dv1U-S9B"}},{"cell_type":"markdown","metadata":{"id":"WtZ6bQFyl2Y9"},"source":["<div>\n","   <img src=\"https://github.com/thomasmanke/ABS/raw/main/figures/BayesEquation.jpg\",  width=\"1000\">\n","</div>"]},{"cell_type":"markdown","source":["## Example: Diagnostic Tests"],"metadata":{"id":"0AfEpe5N98xQ"}},{"cell_type":"markdown","metadata":{"id":"ejtqMnI-99OT"},"source":["<div>\n","   <img src=\"https://github.com/thomasmanke/ABS/raw/main/figures/DiagnosticTest.jpg\",  width=\"1000\">\n","</div>\n"]},{"cell_type":"markdown","metadata":{"id":"iiCGQY-Ns3wY"},"source":["## **Group Tasks (20 min)**: Bayesian Reasoning\n","\n","For the following assume that all HMM parameters are known: $\\pi, P, E$.\n","\n","1. Is the initial distribution the same as the stationary distribution?\n","\n","2. Let's assume that I sent you my (first ever) message, saying that I just had Fondue for dinner. What is the (posterior) probability that I am in Germany?\n","\n","- 5.8%\n","- 50.0%\n","- 75%"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iPK8t7JGwBdo"},"outputs":[],"source":["%%script echo edit before execution\n","# 1. matrix powers\n","from numpy.linalg import matrix_power\n","pi = np.array([1.0, 0.0])         \n","stat_dist = ...   # independent of pi\n","print('stat_dist = ', stat_dist)\n","\n","# 2.  Bayesian analysis\n","sum  = ...\n","prob = ... / sum\n","print('sum = ', sum)\n","print('answer = ', prob)"]},{"cell_type":"markdown","metadata":{"id":"m7LwkNjQxlXN"},"source":["Later you will learn how to incorporate all observations $X$ systematically to derive probabilitic statements for $Z$."]},{"cell_type":"markdown","metadata":{"id":"WNpoLPjgC5Vk"},"source":["# Back to HMM: Hiding Z\n","\n","From now on we assume that the *hidden* state sequence $Z=Z_{1:T}$ is never observed. \n","\n","<div>\n","   <img src=\"https://github.com/thomasmanke/ABS/raw/main/figures/HMM_Chain.jpg\",  width=\"1000\">\n","</div>\n","\n","However, if we know the HMM parameters, we can still give a probabilitic description for $Z$:\n","\n","- **prior probability** $Pr(Z_t=i)‚ü∂$ stationary distribution $\\pi = \\pi P$ \n","- **posterior probability** $Pr(Z_t=i|X_t=k) \\longrightarrow$ Bayesian update: $ \\propto Pr(X_t=k|Z_t=i) Pr(Z_t=i)$\n","- **most likely hidden state sequence** for given observations: $argmax_Z P(Z|X) \\longrightarrow$ Viterbi algorithm\n","\n","\n","\n"," "]},{"cell_type":"markdown","metadata":{"id":"rD7kUxU4rKho"},"source":["# Reference: Conventions and Notations"]},{"cell_type":"markdown","metadata":{"id":"CLO6ncMQrRYj"},"source":["- Number of States: $i=1,2, \\ldots N$\n","- Number of Observations (discrete values): $k=1,2, \\ldots M$\n","- Length of Sequence: $t=1,2, \\ldots T$\n","- conditional probabilities (notice the index order !)\n","  - $P_{ij} = Pr(Z_{t+1}=j|Z_t=i)$ \n","  - $E_{ik} = Pr(X_{t}=k|Z_t=i)$ \n","- Transition Matrix: $P=(P_{ij})$ $\\longrightarrow N \\times N$ matrix\n","- Emission Matrix: $E=(E_{ik})$ $\\longrightarrow N \\times M$ matrix\n","- Initial Probability: $\\pi_i$  $\\longrightarrow N$ dim. row vector\n","- Sequences: condensed notation\n","  - observations $X = X_{1:T} = (X_1, X_2, \\ldots, X_T)$\n","  - hidden states $Z = Z_{1:T} = (Z_1, Z_2, \\ldots, Z_T)$\n","\n","\n","**Notice:** \n","\n","- Both states and observations are discrete variables (e.g. $Z=GGGGIIIIGGGG...$ and $X=ACTGTCGCGCGATTA$) but they are often encoded as integer variables, e.g. $Z=000011110000$\n","- more condensed notation: $Pr(Z_t=i) = Pr(Z_t)$\n","- all $X_t$ are observed, so $E_{ik}$ serves as a look-up table. Sometimes I write $E_{it}$ \n","- Python indices of arrays start at 0"]},{"cell_type":"markdown","metadata":{"id":"m6zZIR6xfSi-"},"source":["# Joint Probability: $Pr(X,Z)$"]},{"cell_type":"markdown","metadata":{"id":"g13M5yh-XNSE"},"source":["Hidden Markov Models are graphical models:\n","\n","\n","<div>\n","   <img src=\"https://github.com/thomasmanke/ABS/raw/main/figures/HMM_Joint.jpg\",  width=\"1000\">\n","</div>\n"]},{"cell_type":"markdown","metadata":{"id":"H8QN7xLZfd0l"},"source":["$$ \n","Pr(X,Z) = Pr(X|Z) Pr(Z) = Pr(Z_1) Pr(X_1|Z_1) \\prod_{t=2}^T Pr(Z_t | Z_{t-1}) Pr(X_{t}|Z_t)\n","$$\n"]},{"cell_type":"markdown","metadata":{"id":"MbD4MwHTXVnK"},"source":["**Message**: \n","- Chain together probabilities of initial state, state transitions and observed emissions!\n","- For given sequences ($X$ and $Z$), multiply all edge probabilities in graphical model ! \n","- **Recursion Principle:** if partial solution $Pr(X_{1:t}, Z_{1:t})$ is available,  $Pr(X_{1:t+1}, Z_{1:t+1})$ can be obtained iteratively"]},{"cell_type":"markdown","metadata":{"id":"bT6xv74T87fi"},"source":["**Notice:** \n","- Condensed notation, e.g. $Pr(X,Z) = Pr(X_1=x_1, X_2=x_2, \\ldots, Z_1=z_1, Z_2=z_2, \\ldots, Z_T=z_T)$ \n","- $\\sum_X \\sum_Z P(X,Z) = 1$\n","- for discrete Markov chains both $X_t$ and $Z_t$ are discrete variables. Their values are often represented by integers, e.g. $X=(0,0,2,1,1,2,2, \\ldots)$\n","- Strictly we may want to write $Pr(X,Z|\\Theta)$ to highlight the conditioning on known parameters $\\Theta=(P,E,\\pi)$. For now we assume that those parameters are all known and fixed."]},{"cell_type":"markdown","metadata":{"id":"xlvj-Feu8XJV"},"source":["## Discussion (10 min):\n","\n","- Why would we want to know the joint distribution $P(X,Z)$ ? Why will it be difficult ?\n","- How many possible sequences are there for a) observations $X$ and b) hidden states $Z$?\n","- What happens to $Pr(X,Z)$ if the sequence $Z$ contains forbidden transitions?"]},{"cell_type":"markdown","metadata":{"id":"XVK9rPUBEF_r"},"source":["## Group Task (20 min)\n","\n","Given the HMM parameters and an observed sequence $X=(0,0,2)$. You have not observed the corresponding sequence $Z=(z_1, z_2, z_3)$, but given the HMM parameters you can calculate the probability of all possible hidden state paths $Z$\n","- Group 1: all paths starting with $G$\n","- Group 2: all paths starting with $S$\n","\n","Report back the path with the highest probability and compare.\n","What happens if you sum all collected probabilities?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Br2gG6oDFCWh","outputId":"b23df218-43d5-4aa7-8956-da54b94fe0c6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655065182743,"user_tz":-120,"elapsed":4,"user":{"displayName":"Thomas Manke","userId":"17591636328965298454"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["edit before execution\n"]}],"source":["%%script echo edit before execution\n","print('0 0 0: ', pi[0]*E[...]]*P[...]*E[...]*...)\n","...\n","\n","# alternatively write a loop over Z\n","X = [0,0,2]\n","\n"]},{"cell_type":"markdown","metadata":{"id":"e_-ZrBAZ08wp"},"source":["# Group Task (30 min): HMM Generation"]},{"cell_type":"markdown","metadata":{"id":"zhG8u7wX1BjT"},"source":["1. Make up your own hidden Markov story, draw the corresponding state graph, and define the Hidden Markov Model. \n","  - Please keep it simple; less than 5 hidden states and less than 5 possible observations. \n","  - Also make sure that the Markov Model for the hidden states is *ergodic* (what was that?)\n","\n","2. Choose your own emission probabilties, transition probabilties and the initital state distribution - make sure they correspond to probabilties. \n","\n","3. Simulate $T=1000$ steps.\n","\n","4. Record (only) the sequence of observations that were generated and store the results as string in a text file (for latter use). Be kind and use integer encoding of observations, i.e. $0,1,\\ldots$ regardless of the interpretation.\n","\n","5. Share your story, code, results and report back to the class."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":228,"status":"ok","timestamp":1652980140145,"user":{"displayName":"Thomas Manke","userId":"17591636328965298454"},"user_tz":-120},"id":"DaVQuGjh6Yey","outputId":"d2d7db31-da90-4508-df06-c974322682e2","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["X = 1 1 0 1 0 1 0 0 1 0 1\n","Xr= 1 1 0 1 0 1 0 0 1 0 1\n"]},{"data":{"text/plain":["True"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["%%script echo edit before execution (only one per group)\n","\n","pi=np.array( ... )\n","P =np.array( ... )\n","E =np.array( ... )\n","\n","T=1000\n","Z, X = generate_HMM(P, pi, E, T=T)\n","\n","fn='obs_group1.txt'  # choose a group-specific filename\n","\n","# write ######\n","with open(fn, 'w') as f:\n","  m = map(str, X)            # convert numbers to strings\n","  f.write(' '.join(list(m))) \n","\n","# read (for later) ######\n","with open(fn, \"r\") as f:\n","  line  = f.readline().split()  # read first line and split\n","Xr = list(map(np.int64, line))  # map line to np.int64\n","\n","#print('X =',*X)\n","#print('Xr=',*Xr)\n","np.all(X==Xr)"]},{"cell_type":"markdown","metadata":{"id":"Cq0uHXT6nWKG"},"source":["# Applications: What is this all good for?"]},{"cell_type":"markdown","metadata":{"id":"X7IOq7c3nZSe"},"source":["- Speech Recognition\n","- DNA sequence analysis: sequence segmentation\n","- Robot Location\n"]},{"cell_type":"markdown","metadata":{"id":"EUS7uJgwbK4o"},"source":["# Typical Problems"]},{"cell_type":"markdown","metadata":{"id":"Q5pQh27N4vz6"},"source":["Model parameters: $\\Theta$, Observations: $X$, Hidden States: $Z$\n","\n","- scoring (observations): $Pr(X) \\longrightarrow$ **Forward Algorithm**\n","\n","- decoding (hidden variables):\n","    - best posterior state: $argmax_i Pr(Z_t=i|X) \\longrightarrow$  **Forward-Backward Algorithm**\n","    - best state sequence: $argmax_Z Pr(Z|X) \\longrightarrow$ **Viterbi Algorithm**\n","    \n","- learning (model parameters)  $argmax_\\Theta Pr(\\Theta|X) \\longrightarrow$ **Baum-Welch Algorithm**"]}],"metadata":{"colab":{"collapsed_sections":["2q91dv1U-S9B","0AfEpe5N98xQ"],"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":0}